{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"OGC \u00b6 ogc - provisioning, that's it. Getting Started \u00b6 Welcome to the getting started guide! This should be a quick introduction to get up and running with OGC. More information on customizing and extending OGC can be found in the user documentation. Install \u00b6 We use and recommend the use of Poetry : $ pip install poetry $ poetry install Caution If using poetry make sure to prefix running of ogc with the following: $ poetry run ogc Optionally, load up the virtualenv beforehand: $ poetry shell Or install from pypi : $ pip install ogc Initialize \u00b6 Next is to initialize the OGC environment, to do that run: $ ogc init It will ask you for a name, feel free to put something other than your actual name if desired. Provider Setup \u00b6 OGC currently supports AWS and GCP out of the box (more added soon). In order for OGC to connect and deploy to these clouds a few environment variables are needed. Create a .env file in the top level directory where ogc is to be run: AWS_ACCESS_KEY_ID=\"\" AWS_SECRET_ACCESS_KEY=\"\" AWS_REGION=\"us-east-2\" GOOGLE_APPLICATION_CREDENTIALS=\"svc.json\" GOOGLE_APPLICATION_SERVICE_ACCOUNT=\"..@...iam.gserviceaccount.com\" GOOGLE_PROJECT=\"example-project\" GOOGLE_DATACENTER=\"us-central1-a\" Note More information can be found in our Providers documentation. Define Provisioning \u00b6 Once setup is complete, a provision specification is needed. This defines ssh-keys and one or more layouts to be provisioned. Create a file ogc.toml and place in the top level directory where ogc is run: name = \"ci\" [ssh-keys] private = \"~/.ssh/id_rsa_libcloud\" public = \"~/.ssh/id_rsa_libcloud.pub\" [layouts.elastic-agent-ubuntu] artifacts = \"/home/ubuntu/output/*.xml\" exclude = [ \".git\" , \".venv\" , \"artifacts\" ] extra = { } include = [ ] instance-size = \"e2-standard-4\" ports = [ \"22:22\" , \"80:80\" , \"443:443\" , \"5601:5601\" ] provider = \"google\" remote-path = \"/home/ubuntu/ogc\" runs-on = \"ubuntu-2004-lts\" scale = 1 scripts = \"fixtures/ex_deploy_ubuntu\" tags = [ \"elastic-agent-8-1-x\" , \"ubuntu-gcp\" ] username = \"ubuntu\" This specification tells OGC to deploy 5 nodes running on Google's e2-standard-8 with Ubuntu OS. The scripts section tells OGC where the template files/scripts are located that need to be uploaded to each node during the deployment phase. Provision and Deploy \u00b6 Once the specification is set, environment variables configured and a postgres database is accessible, execute a deployment in a new terminal: $ ogc launch Note If the file is something other than ogc.toml append the --spec option to the launch command: $ ogc launch --spec my-custom-provision.toml Next steps \u00b6 Learn how to manage your deployments in our User Guide - Managing a deployment","title":"Getting Started"},{"location":"#ogc","text":"ogc - provisioning, that's it.","title":"OGC"},{"location":"#getting-started","text":"Welcome to the getting started guide! This should be a quick introduction to get up and running with OGC. More information on customizing and extending OGC can be found in the user documentation.","title":"Getting Started"},{"location":"#install","text":"We use and recommend the use of Poetry : $ pip install poetry $ poetry install Caution If using poetry make sure to prefix running of ogc with the following: $ poetry run ogc Optionally, load up the virtualenv beforehand: $ poetry shell Or install from pypi : $ pip install ogc","title":"Install"},{"location":"#initialize","text":"Next is to initialize the OGC environment, to do that run: $ ogc init It will ask you for a name, feel free to put something other than your actual name if desired.","title":"Initialize"},{"location":"#provider-setup","text":"OGC currently supports AWS and GCP out of the box (more added soon). In order for OGC to connect and deploy to these clouds a few environment variables are needed. Create a .env file in the top level directory where ogc is to be run: AWS_ACCESS_KEY_ID=\"\" AWS_SECRET_ACCESS_KEY=\"\" AWS_REGION=\"us-east-2\" GOOGLE_APPLICATION_CREDENTIALS=\"svc.json\" GOOGLE_APPLICATION_SERVICE_ACCOUNT=\"..@...iam.gserviceaccount.com\" GOOGLE_PROJECT=\"example-project\" GOOGLE_DATACENTER=\"us-central1-a\" Note More information can be found in our Providers documentation.","title":"Provider Setup"},{"location":"#define-provisioning","text":"Once setup is complete, a provision specification is needed. This defines ssh-keys and one or more layouts to be provisioned. Create a file ogc.toml and place in the top level directory where ogc is run: name = \"ci\" [ssh-keys] private = \"~/.ssh/id_rsa_libcloud\" public = \"~/.ssh/id_rsa_libcloud.pub\" [layouts.elastic-agent-ubuntu] artifacts = \"/home/ubuntu/output/*.xml\" exclude = [ \".git\" , \".venv\" , \"artifacts\" ] extra = { } include = [ ] instance-size = \"e2-standard-4\" ports = [ \"22:22\" , \"80:80\" , \"443:443\" , \"5601:5601\" ] provider = \"google\" remote-path = \"/home/ubuntu/ogc\" runs-on = \"ubuntu-2004-lts\" scale = 1 scripts = \"fixtures/ex_deploy_ubuntu\" tags = [ \"elastic-agent-8-1-x\" , \"ubuntu-gcp\" ] username = \"ubuntu\" This specification tells OGC to deploy 5 nodes running on Google's e2-standard-8 with Ubuntu OS. The scripts section tells OGC where the template files/scripts are located that need to be uploaded to each node during the deployment phase.","title":"Define Provisioning"},{"location":"#provision-and-deploy","text":"Once the specification is set, environment variables configured and a postgres database is accessible, execute a deployment in a new terminal: $ ogc launch Note If the file is something other than ogc.toml append the --spec option to the launch command: $ ogc launch --spec my-custom-provision.toml","title":"Provision and Deploy"},{"location":"#next-steps","text":"Learn how to manage your deployments in our User Guide - Managing a deployment","title":"Next steps"},{"location":"commands/ogc/","text":"CLI Reference \u00b6 This page provides documentation for our command line tool. ogc \u00b6 Just a simple provisioner Usage: ogc [OPTIONS] COMMAND [ARGS]... Options: Name Type Description Default --help boolean Show this message and exit. False ogc exec \u00b6 Execute a command across node(s) Usage: ogc exec [OPTIONS] CMD Options: Name Type Description Default --by-tag text Only run on nodes matching tag None --by-name text Only run on nodes matching name None --help boolean Show this message and exit. False ogc exec-scripts \u00b6 (R)Execute a set of scripts Usage: ogc exec-scripts [OPTIONS] PATH Options: Name Type Description Default --by-tag text Only run on nodes matching tag None --by-name text Only run on nodes matching name None --help boolean Show this message and exit. False ogc export-env \u00b6 Exports the deployment to be shared with other users This exports the current database and imports the public ssh key of the shared user. Note on cloud credentials: They are not exported and must be set on the shared users environment. Usage: ogc export-env [OPTIONS] Options: Name Type Description Default --spec text Location of the ogc.toml or other spec files to include in export None --db-file text Filename of the database dump ogc-dump.sql --env-file text Filename of where to store the OGC environment to be shared ogc-env.json --help boolean Show this message and exit. False ogc import-env \u00b6 Imports a shared deployment Usage: ogc import-env [OPTIONS] Options: Name Type Description Default --db-file text Filename of the database dump ogc-dump.sql --env-file text Filename of where to store the OGC environment to be shared ogc-env.json --private-ssh-key text The path to your private ssh key. This must match the public ssh key used during the export. _required --public-ssh-key text The path to your public ssh key. This must match the ssh key used during the export. _required --help boolean Show this message and exit. False ogc inspect \u00b6 List nodes in your inventory Usage: ogc inspect [OPTIONS] Options: Name Type Description Default --by-id text Inspect node by DB ID None --by-name text Inspect nodes by name, this can be a substring match None --by-tag text Inspect nodes by tag None --action-id text If set will only show the action output for a specific action ID None --extend / --no-extend boolean Show extended action output at once False --help boolean Show this message and exit. False ogc launch \u00b6 Launches nodes from a provision specification Usage: ogc launch [OPTIONS] Options: Name Type Description Default --spec text N/A None --with-deploy / --with-no-deploy boolean Also performs script deployments True --help boolean Show this message and exit. False ogc ls \u00b6 List nodes in your inventory Usage: ogc ls [OPTIONS] Options: Name Type Description Default --by-tag text List nodes by tag None --by-name text List nodes by name, this can be a substring match None --output-file text Stores the table output to svg or html. Determined by the file extension. None --help boolean Show this message and exit. False ogc pull-artifacts \u00b6 Download artifacts from node Usage: ogc pull-artifacts [OPTIONS] NAME Options: Name Type Description Default --help boolean Show this message and exit. False ogc pull-files \u00b6 Scp files or directories from node Usage: ogc pull-files [OPTIONS] NAME DST SRC Options: Name Type Description Default --help boolean Show this message and exit. False ogc push-files \u00b6 Scp files or directories to node Usage: ogc push-files [OPTIONS] NAME SRC DST Options: Name Type Description Default --exclude text Exclude files/directories when uploading None --help boolean Show this message and exit. False ogc rm \u00b6 Destroys a node and its associated keys, storage, etc. Usage: ogc rm [OPTIONS] Options: Name Type Description Default --by-name text Remove node by its Name None --force / --no-force boolean Force removal regardless of connectivity False --only-db / --no-only-db boolean Force removal of database records only False --help boolean Show this message and exit. False ogc rm-all \u00b6 Destroys everything. Use with caution. Usage: ogc rm-all [OPTIONS] Options: Name Type Description Default --force / --no-force boolean Force removal regardless of connectivity False --only-db / --no-only-db boolean Force removal of database records only False --help boolean Show this message and exit. False ogc shell \u00b6 Launches IPython REPL Usage: ogc shell [OPTIONS] Options: Name Type Description Default --help boolean Show this message and exit. False ogc ssh \u00b6 Login to a node Usage: ogc ssh [OPTIONS] Options: Name Type Description Default --by-id text Login to a node by its ID None --by-name text Login to a node by its Name None --help boolean Show this message and exit. False ogc status \u00b6 Get status of deployment Usage: ogc status [OPTIONS] Options: Name Type Description Default --reconcile / --no-reconcile boolean Attempt to fix deployment to match scale False --spec text N/A None --output-file text Stores the table output to svg or html. Determined by the file extension. None --help boolean Show this message and exit. False","title":"CLI Reference"},{"location":"commands/ogc/#cli-reference","text":"This page provides documentation for our command line tool.","title":"CLI Reference"},{"location":"commands/ogc/#ogc","text":"Just a simple provisioner Usage: ogc [OPTIONS] COMMAND [ARGS]... Options: Name Type Description Default --help boolean Show this message and exit. False","title":"ogc"},{"location":"commands/ogc/#ogc-exec","text":"Execute a command across node(s) Usage: ogc exec [OPTIONS] CMD Options: Name Type Description Default --by-tag text Only run on nodes matching tag None --by-name text Only run on nodes matching name None --help boolean Show this message and exit. False","title":"exec"},{"location":"commands/ogc/#ogc-exec-scripts","text":"(R)Execute a set of scripts Usage: ogc exec-scripts [OPTIONS] PATH Options: Name Type Description Default --by-tag text Only run on nodes matching tag None --by-name text Only run on nodes matching name None --help boolean Show this message and exit. False","title":"exec-scripts"},{"location":"commands/ogc/#ogc-export-env","text":"Exports the deployment to be shared with other users This exports the current database and imports the public ssh key of the shared user. Note on cloud credentials: They are not exported and must be set on the shared users environment. Usage: ogc export-env [OPTIONS] Options: Name Type Description Default --spec text Location of the ogc.toml or other spec files to include in export None --db-file text Filename of the database dump ogc-dump.sql --env-file text Filename of where to store the OGC environment to be shared ogc-env.json --help boolean Show this message and exit. False","title":"export-env"},{"location":"commands/ogc/#ogc-import-env","text":"Imports a shared deployment Usage: ogc import-env [OPTIONS] Options: Name Type Description Default --db-file text Filename of the database dump ogc-dump.sql --env-file text Filename of where to store the OGC environment to be shared ogc-env.json --private-ssh-key text The path to your private ssh key. This must match the public ssh key used during the export. _required --public-ssh-key text The path to your public ssh key. This must match the ssh key used during the export. _required --help boolean Show this message and exit. False","title":"import-env"},{"location":"commands/ogc/#ogc-inspect","text":"List nodes in your inventory Usage: ogc inspect [OPTIONS] Options: Name Type Description Default --by-id text Inspect node by DB ID None --by-name text Inspect nodes by name, this can be a substring match None --by-tag text Inspect nodes by tag None --action-id text If set will only show the action output for a specific action ID None --extend / --no-extend boolean Show extended action output at once False --help boolean Show this message and exit. False","title":"inspect"},{"location":"commands/ogc/#ogc-launch","text":"Launches nodes from a provision specification Usage: ogc launch [OPTIONS] Options: Name Type Description Default --spec text N/A None --with-deploy / --with-no-deploy boolean Also performs script deployments True --help boolean Show this message and exit. False","title":"launch"},{"location":"commands/ogc/#ogc-ls","text":"List nodes in your inventory Usage: ogc ls [OPTIONS] Options: Name Type Description Default --by-tag text List nodes by tag None --by-name text List nodes by name, this can be a substring match None --output-file text Stores the table output to svg or html. Determined by the file extension. None --help boolean Show this message and exit. False","title":"ls"},{"location":"commands/ogc/#ogc-pull-artifacts","text":"Download artifacts from node Usage: ogc pull-artifacts [OPTIONS] NAME Options: Name Type Description Default --help boolean Show this message and exit. False","title":"pull-artifacts"},{"location":"commands/ogc/#ogc-pull-files","text":"Scp files or directories from node Usage: ogc pull-files [OPTIONS] NAME DST SRC Options: Name Type Description Default --help boolean Show this message and exit. False","title":"pull-files"},{"location":"commands/ogc/#ogc-push-files","text":"Scp files or directories to node Usage: ogc push-files [OPTIONS] NAME SRC DST Options: Name Type Description Default --exclude text Exclude files/directories when uploading None --help boolean Show this message and exit. False","title":"push-files"},{"location":"commands/ogc/#ogc-rm","text":"Destroys a node and its associated keys, storage, etc. Usage: ogc rm [OPTIONS] Options: Name Type Description Default --by-name text Remove node by its Name None --force / --no-force boolean Force removal regardless of connectivity False --only-db / --no-only-db boolean Force removal of database records only False --help boolean Show this message and exit. False","title":"rm"},{"location":"commands/ogc/#ogc-rm-all","text":"Destroys everything. Use with caution. Usage: ogc rm-all [OPTIONS] Options: Name Type Description Default --force / --no-force boolean Force removal regardless of connectivity False --only-db / --no-only-db boolean Force removal of database records only False --help boolean Show this message and exit. False","title":"rm-all"},{"location":"commands/ogc/#ogc-shell","text":"Launches IPython REPL Usage: ogc shell [OPTIONS] Options: Name Type Description Default --help boolean Show this message and exit. False","title":"shell"},{"location":"commands/ogc/#ogc-ssh","text":"Login to a node Usage: ogc ssh [OPTIONS] Options: Name Type Description Default --by-id text Login to a node by its ID None --by-name text Login to a node by its Name None --help boolean Show this message and exit. False","title":"ssh"},{"location":"commands/ogc/#ogc-status","text":"Get status of deployment Usage: ogc status [OPTIONS] Options: Name Type Description Default --reconcile / --no-reconcile boolean Attempt to fix deployment to match scale False --spec text N/A None --output-file text Stores the table output to svg or html. Determined by the file extension. None --help boolean Show this message and exit. False","title":"status"},{"location":"developer-guide/managing-nodes/","text":"Managing Nodes Programatically \u00b6 Requirements \u00b6 Accessing the functionality of OGC programatically requires that both cloud credentials and database access are configured. The environment variables for working with AWS or Google should be defined in your environment either by setting it in the .env or in the abscence of a dotenv file they can be exported by your current running shell. Using the .env is easiest and is what we'll use for the remaining documentation, the following will configure access to both AWS and Google along with defining where our Postgres database resides: AWS_ACCESS_KEY_ID = \"abbcc\" AWS_SECRET_ACCESS_KEY = \"sshitsasecret\" AWS_REGION = \"us-east-2\" GOOGLE_APPLICATION_CREDENTIALS = \"mycreds.json\" GOOGLE_APPLICATION_SERVICE_ACCOUNT = \"bob@whodunit.iam.gserviceaccount.com\" GOOGLE_PROJECT = \"my-awesome-project\" GOOGLE_DATACENTER = \"us-central1-a\" Create User \u00b6 A single user record is required in the database, this allows OGC to track cloud resources by certain tags associated with the OGC user. To create an initial user: from ogc import models from ogc.db import M , model_as_pickle user = models . User ( name = name ) with M . db . begin ( write = True ) as txn : if txn . get ( user . slug . encode ( \"ascii\" )): log . warning ( \"OGC already setup.\" ) sys . exit ( 1 ) txn . put ( user . slug . encode ( \"ascii\" ), model_as_pickle ( user )) Querying the database uses standard SQLAlchemy , please reference that site for additional information. Nodes \u00b6 Once the database is setup in your code, you are ready to begin creating and managing nodes. OGC provides both synchronous and asynchronous support depending on your needs. Launch Node \u00b6 To launch a node an OGC specification is required with at least one layout defined. from ogc.spec import SpecLoader app . spec = SpecLoader . load ([ \"/Users/adam/specs/ogc.yml\" ]) To launch this node layout synchronously: from ogc import actions node_ids_created = [ actions . launch ( layout . as_dict ()) for layout in app . spec . layouts ] For an asynchronous version: from ogc import actions node_ids_created = actions . launch_async ( app . spec . layouts ) Info The naming conventions used for async functions is to append the suffix of async to the synchronous function name, for example, actions.exec and actions.exec_async . Script Deployment \u00b6 Launching and deploying are separated into two parts, this allows for further customization between bringing up a machine and letting OGC handle the remaining deployment options. To deploy the scripts defined in your specification, use the results from the previous launch of node_ids_created : script_deploy_results = actions . deploy_async ( node_ids_created ) Checking Results \u00b6 Checking the results of the deployment can be done in this way: if all ( result == True for result in script_deploy_results ): print ( \"Successfully deployed\" ) else : print ( \"One or more deployments failed\" )","title":"Managing nodes"},{"location":"developer-guide/managing-nodes/#managing-nodes-programatically","text":"","title":"Managing Nodes Programatically"},{"location":"developer-guide/managing-nodes/#requirements","text":"Accessing the functionality of OGC programatically requires that both cloud credentials and database access are configured. The environment variables for working with AWS or Google should be defined in your environment either by setting it in the .env or in the abscence of a dotenv file they can be exported by your current running shell. Using the .env is easiest and is what we'll use for the remaining documentation, the following will configure access to both AWS and Google along with defining where our Postgres database resides: AWS_ACCESS_KEY_ID = \"abbcc\" AWS_SECRET_ACCESS_KEY = \"sshitsasecret\" AWS_REGION = \"us-east-2\" GOOGLE_APPLICATION_CREDENTIALS = \"mycreds.json\" GOOGLE_APPLICATION_SERVICE_ACCOUNT = \"bob@whodunit.iam.gserviceaccount.com\" GOOGLE_PROJECT = \"my-awesome-project\" GOOGLE_DATACENTER = \"us-central1-a\"","title":"Requirements"},{"location":"developer-guide/managing-nodes/#create-user","text":"A single user record is required in the database, this allows OGC to track cloud resources by certain tags associated with the OGC user. To create an initial user: from ogc import models from ogc.db import M , model_as_pickle user = models . User ( name = name ) with M . db . begin ( write = True ) as txn : if txn . get ( user . slug . encode ( \"ascii\" )): log . warning ( \"OGC already setup.\" ) sys . exit ( 1 ) txn . put ( user . slug . encode ( \"ascii\" ), model_as_pickle ( user )) Querying the database uses standard SQLAlchemy , please reference that site for additional information.","title":"Create User"},{"location":"developer-guide/managing-nodes/#nodes","text":"Once the database is setup in your code, you are ready to begin creating and managing nodes. OGC provides both synchronous and asynchronous support depending on your needs.","title":"Nodes"},{"location":"developer-guide/managing-nodes/#launch-node","text":"To launch a node an OGC specification is required with at least one layout defined. from ogc.spec import SpecLoader app . spec = SpecLoader . load ([ \"/Users/adam/specs/ogc.yml\" ]) To launch this node layout synchronously: from ogc import actions node_ids_created = [ actions . launch ( layout . as_dict ()) for layout in app . spec . layouts ] For an asynchronous version: from ogc import actions node_ids_created = actions . launch_async ( app . spec . layouts ) Info The naming conventions used for async functions is to append the suffix of async to the synchronous function name, for example, actions.exec and actions.exec_async .","title":"Launch Node"},{"location":"developer-guide/managing-nodes/#script-deployment","text":"Launching and deploying are separated into two parts, this allows for further customization between bringing up a machine and letting OGC handle the remaining deployment options. To deploy the scripts defined in your specification, use the results from the previous launch of node_ids_created : script_deploy_results = actions . deploy_async ( node_ids_created )","title":"Script Deployment"},{"location":"developer-guide/managing-nodes/#checking-results","text":"Checking the results of the deployment can be done in this way: if all ( result == True for result in script_deploy_results ): print ( \"Successfully deployed\" ) else : print ( \"One or more deployments failed\" )","title":"Checking Results"},{"location":"developer-guide/api/actions/","text":"API \u00b6 ogc.actions \u00b6 exec ( node , cmd ) \u00b6 Execute command on Node Function for executing a command on a node. Synopsis: from ogc import actions , state , db node = db . get_nodes () . unwrap ()[ 0 ] actions . exec ( node , \"ls -l /\" ) for action in node . actions : print ( action . exit_code , action . out , action . error ) Parameters: Name Type Description Default node models.Node The models.Node to execute a command on required cmd str The command string required Returns: Type Description bool True if succesful, False otherwise exec_async ( name , tag , cmd ) \u00b6 Execute command on Nodes Async function for executing a command on a node. Synopsis: from ogc import actions , state , db node = db . query ( db . Node ) . filter ( db . Node . tags . contains ([ tag ])) results = actions . exec_async ( node , \"ls -l /\" ) all ( result == True for result in results ) Parameters: Name Type Description Default name str The node name to execute a command on required tag str The tag to query for nodes. Allows running commands across multiple nodes. required cmd str The command string required Returns: Type Description list[bool] True if all execs complete succesfully, False otherwise. exec_scripts ( node , path ) \u00b6 Execute a scripts/template directory on a Node Function for executing scripts/templates on a node. Synopsis: from ogc import actions , state , db node = db . query ( db . Node ) . first () result = actions . exec_scripts ( node , \"templates/deploy/ubuntu\" ) result == True Parameters: Name Type Description Default node bytes The Pickled models.Node to execute scripts on required path str The path where the scripts reside locally required Returns: Type Description bytes Pickled models.Node if succesful, False otherwise. exec_scripts_async ( path , filters = None ) \u00b6 Execute a scripts/template directory on a Node Async function for executing scripts/templates on a node. Synopsis: from ogc import actions , state , db results = actions . exec_scripts_async ( path = \"templates/deploy/ubuntu\" , filters = { \"name\" : \"ogc-1\" }) all ( result == True for result in results ) Parameters: Name Type Description Default path str The path where the scripts reside locally required filters Mapping[str, str] Filters to pass into exec, currently name and tag are supported. None Returns: Type Description list[models.Node] models.Node if succesful, False otherwise. launch ( layout ) \u00b6 Launch a node. Synchronous function for launching a node in a cloud environment. Synopsis: from ogc.spec import SpecLoader from ogc import actions , db spec = SpecLoader . load ([ \"/Users/adam/specs/ogc.toml\" ]) nodes = [ actions . launch ( layout ) for layout in spec . layouts ] Parameters: Name Type Description Default layout bytes models.Layout specification used when launching a node. required Returns: Type Description bytes Pickled models.Node Instance if successful, None otherwise. launch_async ( layouts , with_deploy = True ) \u00b6 Launch a node asynchronously. Asynchronous function for launching a node in a cloud environment. Synopsis: from ogc.spec import SpecLoader from ogc import actions , db , models spec = SpecLoader . load ([ \"/Users/adam/specs/ogc.toml\" ]) nodes = actions . launch_async ( layouts = spec . layouts ) Parameters: Name Type Description Default layouts list[models.Layout] The layout specification used when launching a node. required with_deploy bool Also execute deployment scripts True Returns: Type Description list[models.Node] List of instance names launched sync ( layout , overrides ) \u00b6 Sync a deployment Function for syncing a deployment to correct scale. Synopsis: from ogc import actions , state layout = app . spec . layouts [ 0 ] result = actions . sync ( layout , overrides = { 'elastic-agent-ubuntu' : { 'action' : 'add' , remaining : 5 }}) result == True Parameters: Name Type Description Default layout bytes The Pickled models.Layout required overrides dict[str, str] Override dictionary of what the new count of nodes should be required Returns: Type Description bytes True if synced, False otherwise sync_async ( layouts , overrides ) \u00b6 Sync a deployment Async function for syncing a deployment to correct scale. Synopsis: from ogc import actions , state results = actions . sync_async ( app . spec . layouts , overrides = { 'elastic-agent-ubuntu' : { 'action' : 'add' , remaining : 5 }}) all ( result == True for result in results ) Parameters: Name Type Description Default layouts list[models.Layout] The list of models.Layout of the deployment required overrides dict Override dictionary of what the new count of nodes should be required Returns: Type Description bool True if synced, False otherwise teardown ( node , force = False , only_db = False ) \u00b6 Teardown deployment Function for tearing down a node. Synopsis: from ogc import actions name = \"ogc-234342-elastic-agent-ubuntu\" is_down = actions . teardown ( name , force = True ) Parameters: Name Type Description Default node bytes Pickled models.Node instance required force bool Force False only_db bool Will remove from database regardless of cloud state. Use with caution. False Returns: Type Description bytes Pickled models.Node that was removed, None otherwise. teardown_async ( nodes , force = False , only_db = False ) \u00b6 Teardown deployment Async function for tearing down a node. Synopsis: from ogc import actions , db result = actions . teardown_async ( db . M . get_nodes () . unwrap (), force = True ) assert ( len ( result ) > 0 ) Parameters: Name Type Description Default nodes list[models.Node] The node name to teardown required force bool Force False only_db bool Will remove from database regardless of cloud state. Use with caution. False Returns: Type Description list[models.Node] List of instances destroyed","title":"ogc"},{"location":"developer-guide/api/actions/#api","text":"","title":"API"},{"location":"developer-guide/api/actions/#ogc.actions","text":"","title":"actions"},{"location":"developer-guide/api/actions/#ogc.actions.exec","text":"Execute command on Node Function for executing a command on a node. Synopsis: from ogc import actions , state , db node = db . get_nodes () . unwrap ()[ 0 ] actions . exec ( node , \"ls -l /\" ) for action in node . actions : print ( action . exit_code , action . out , action . error ) Parameters: Name Type Description Default node models.Node The models.Node to execute a command on required cmd str The command string required Returns: Type Description bool True if succesful, False otherwise","title":"exec()"},{"location":"developer-guide/api/actions/#ogc.actions.exec_async","text":"Execute command on Nodes Async function for executing a command on a node. Synopsis: from ogc import actions , state , db node = db . query ( db . Node ) . filter ( db . Node . tags . contains ([ tag ])) results = actions . exec_async ( node , \"ls -l /\" ) all ( result == True for result in results ) Parameters: Name Type Description Default name str The node name to execute a command on required tag str The tag to query for nodes. Allows running commands across multiple nodes. required cmd str The command string required Returns: Type Description list[bool] True if all execs complete succesfully, False otherwise.","title":"exec_async()"},{"location":"developer-guide/api/actions/#ogc.actions.exec_scripts","text":"Execute a scripts/template directory on a Node Function for executing scripts/templates on a node. Synopsis: from ogc import actions , state , db node = db . query ( db . Node ) . first () result = actions . exec_scripts ( node , \"templates/deploy/ubuntu\" ) result == True Parameters: Name Type Description Default node bytes The Pickled models.Node to execute scripts on required path str The path where the scripts reside locally required Returns: Type Description bytes Pickled models.Node if succesful, False otherwise.","title":"exec_scripts()"},{"location":"developer-guide/api/actions/#ogc.actions.exec_scripts_async","text":"Execute a scripts/template directory on a Node Async function for executing scripts/templates on a node. Synopsis: from ogc import actions , state , db results = actions . exec_scripts_async ( path = \"templates/deploy/ubuntu\" , filters = { \"name\" : \"ogc-1\" }) all ( result == True for result in results ) Parameters: Name Type Description Default path str The path where the scripts reside locally required filters Mapping[str, str] Filters to pass into exec, currently name and tag are supported. None Returns: Type Description list[models.Node] models.Node if succesful, False otherwise.","title":"exec_scripts_async()"},{"location":"developer-guide/api/actions/#ogc.actions.launch","text":"Launch a node. Synchronous function for launching a node in a cloud environment. Synopsis: from ogc.spec import SpecLoader from ogc import actions , db spec = SpecLoader . load ([ \"/Users/adam/specs/ogc.toml\" ]) nodes = [ actions . launch ( layout ) for layout in spec . layouts ] Parameters: Name Type Description Default layout bytes models.Layout specification used when launching a node. required Returns: Type Description bytes Pickled models.Node Instance if successful, None otherwise.","title":"launch()"},{"location":"developer-guide/api/actions/#ogc.actions.launch_async","text":"Launch a node asynchronously. Asynchronous function for launching a node in a cloud environment. Synopsis: from ogc.spec import SpecLoader from ogc import actions , db , models spec = SpecLoader . load ([ \"/Users/adam/specs/ogc.toml\" ]) nodes = actions . launch_async ( layouts = spec . layouts ) Parameters: Name Type Description Default layouts list[models.Layout] The layout specification used when launching a node. required with_deploy bool Also execute deployment scripts True Returns: Type Description list[models.Node] List of instance names launched","title":"launch_async()"},{"location":"developer-guide/api/actions/#ogc.actions.sync","text":"Sync a deployment Function for syncing a deployment to correct scale. Synopsis: from ogc import actions , state layout = app . spec . layouts [ 0 ] result = actions . sync ( layout , overrides = { 'elastic-agent-ubuntu' : { 'action' : 'add' , remaining : 5 }}) result == True Parameters: Name Type Description Default layout bytes The Pickled models.Layout required overrides dict[str, str] Override dictionary of what the new count of nodes should be required Returns: Type Description bytes True if synced, False otherwise","title":"sync()"},{"location":"developer-guide/api/actions/#ogc.actions.sync_async","text":"Sync a deployment Async function for syncing a deployment to correct scale. Synopsis: from ogc import actions , state results = actions . sync_async ( app . spec . layouts , overrides = { 'elastic-agent-ubuntu' : { 'action' : 'add' , remaining : 5 }}) all ( result == True for result in results ) Parameters: Name Type Description Default layouts list[models.Layout] The list of models.Layout of the deployment required overrides dict Override dictionary of what the new count of nodes should be required Returns: Type Description bool True if synced, False otherwise","title":"sync_async()"},{"location":"developer-guide/api/actions/#ogc.actions.teardown","text":"Teardown deployment Function for tearing down a node. Synopsis: from ogc import actions name = \"ogc-234342-elastic-agent-ubuntu\" is_down = actions . teardown ( name , force = True ) Parameters: Name Type Description Default node bytes Pickled models.Node instance required force bool Force False only_db bool Will remove from database regardless of cloud state. Use with caution. False Returns: Type Description bytes Pickled models.Node that was removed, None otherwise.","title":"teardown()"},{"location":"developer-guide/api/actions/#ogc.actions.teardown_async","text":"Teardown deployment Async function for tearing down a node. Synopsis: from ogc import actions , db result = actions . teardown_async ( db . M . get_nodes () . unwrap (), force = True ) assert ( len ( result ) > 0 ) Parameters: Name Type Description Default nodes list[models.Node] The node name to teardown required force bool Force False only_db bool Will remove from database regardless of cloud state. Use with caution. False Returns: Type Description list[models.Node] List of instances destroyed","title":"teardown_async()"},{"location":"user-guide/defining-layouts/","text":"Defining Layouts \u00b6 Learn the layout specification and how to create your own provisioning layouts. All layouts reside under the layouts key in the provision specification: [layouts.elastic-agent-ubuntu] artifacts = \"/home/ubuntu/output/*.xml\" exclude = [ \".git\" , \".venv\" , \"artifacts\" ] extra = { } include = [ ] instance-size = \"e2-standard-4\" ports = [ \"22:22\" , \"80:80\" , \"443:443\" , \"5601:5601\" ] provider = \"google\" remote-path = \"/home/ubuntu/ogc\" runs-on = \"ubuntu-2004-lts\" scale = 1 scripts = \"fixtures/ex_deploy_ubuntu\" tags = [ \"elastic-agent-8-1-x\" , \"ubuntu-gcp\" ] username = \"ubuntu\" Each layout has a friendly name associated as seen by elastic-agent-ubuntu . The next section is going to go over each option and describe its meaning. provider Define which cloud the layout will operate in. Currently supported options are aws and google . runs-on Define the base OS image to be deployed on to the nodes. The current supported list of names are: AWS Google ubuntu-2004 ubuntu-2004 ubuntu-1804 ubuntu-1804 ubuntu-1604 ubuntu-1604 sles-15 sles-15 sles-12 sles-12 sles-11 sles-11 debian-10 debian-10 debian-9 debian-9 debian-8 debian-8 rhel-8 rhel-7 rhel-6 instance-size Define the machine size, this is dependent on which provider is chosen. The instance-size correlates with the instance size naming for each cloud. For example, on AWS you would use instance-size = \"c5.4xlarge\" and in Google's case, instance-size = \"e2-standard-4\" . username The ssh user to use when deploying and accessing the nodes. This is also somewhat dependent on which provider is used. In the case of Google , any username can be given. In the case of AWS , the base machines have a pre-loaded user that must be used: AWS Username centos centos debian admin oracle ec2-user sles ec2-user ubuntu ubuntu windows 1 ogc Caution A lot of cloud machine images disable root login, try to avoid using that as a user and utilize sudo for anything requiring elevated permissions. scripts The location on your machine where templates/scripts resides. These will be uploaded and executed during the deployment phase. Note See scripting for more information. scale How many nodes of each layout to deploy. This is also referenced during a deployment reconciliation phase. remote-path (optional) If set, any uploads/downloads outside of what's defined in scripts will be placed in that remote path. include (optional) A list of files/wildcards to include in the upload exclude (optional) A list of files/wildcards to exclude in the upload artifacts (optional) The remote path where script execution output is stored. This is used when pulling artifacts with ogc pull-artifacts and also utilized during node teardown. This will download any artifacts found into artifacts/instance-name/ . tags (optional) Define tags for each layout, allows additional filtering capabilities and deployment options when used with ogc ls and ogc exec ports (optional) Define what ingress ports are available when accessing the node. Variants \u00b6 OGC supports the concept of variants. In OGC's case, variants are handled by multiple provision spec files and are then merged in a merge-left fashion. What this means is that we can take a base spec file such as: name = \"ci\" [ssh-keys] public = \"id_rsa.pub\" private = \"id_rsa\" [layouts.elastic-agent-sles] runs-on = \"sles-15\" instance-size = \"e2-standard-8\" username = \"ogc\" scripts = \"fixtures/ex_deploy_sles\" provider = \"google\" scale = 5 remote-path = \"/home/ogc/ogc\" include = [ \".ogc-cache\" ] exclude = [ \".git\" , \".venv\" ] artifacts = \"/home/ogc/output/*.xml\" tags = [ \"elastic-agent-8.1.x\" , \"sles-gcp\" ] [layouts.elastic-agent-ubuntu] runs-on = \"ubuntu-latest\" instance-size = \"e2-standard-8\" username = \"ogc\" scripts = \"fixtures/ex_deploy_ubuntu\" provider = \"google\" scale = 5 remote-path = \"/home/ogc/ogc\" exclude = [ \".git\" , \".venv\" ] artifacts = \"/home/ogc/output/*.xml\" tags = [ \"elastic-agent-8.1.x\" , \"ubuntu-gcp\" ] The name of the file doesn't matter, we'll call this file base-spec.toml . Now if we need to change certain aspects of this base deploy specification we can define a second YAML file, we'll call it ubuntu-1804-no-sles.toml . In this example, let's change the username and runs-on for the ubuntu layout, and let's also remove the sles layout: [layouts] elastic-agent-sles = { } [layouts.elastic-agent-ubuntu] runs-on = \"ubuntu-1804\" username = \"ubuntu\" The merging of the specifications will remove any keys that exist if the value of the key is {} (signaling an empty stanza). The remaining keys that match up with the original spec will then be overridden and the rest of the specification is left untouched. Adding new layouts is just a matter of defining another section under layouts in the additional spec files passed to OGC. This is the default user for our contributed packer build for Windows \u21a9","title":"Defining Layouts"},{"location":"user-guide/defining-layouts/#defining-layouts","text":"Learn the layout specification and how to create your own provisioning layouts. All layouts reside under the layouts key in the provision specification: [layouts.elastic-agent-ubuntu] artifacts = \"/home/ubuntu/output/*.xml\" exclude = [ \".git\" , \".venv\" , \"artifacts\" ] extra = { } include = [ ] instance-size = \"e2-standard-4\" ports = [ \"22:22\" , \"80:80\" , \"443:443\" , \"5601:5601\" ] provider = \"google\" remote-path = \"/home/ubuntu/ogc\" runs-on = \"ubuntu-2004-lts\" scale = 1 scripts = \"fixtures/ex_deploy_ubuntu\" tags = [ \"elastic-agent-8-1-x\" , \"ubuntu-gcp\" ] username = \"ubuntu\" Each layout has a friendly name associated as seen by elastic-agent-ubuntu . The next section is going to go over each option and describe its meaning. provider Define which cloud the layout will operate in. Currently supported options are aws and google . runs-on Define the base OS image to be deployed on to the nodes. The current supported list of names are: AWS Google ubuntu-2004 ubuntu-2004 ubuntu-1804 ubuntu-1804 ubuntu-1604 ubuntu-1604 sles-15 sles-15 sles-12 sles-12 sles-11 sles-11 debian-10 debian-10 debian-9 debian-9 debian-8 debian-8 rhel-8 rhel-7 rhel-6 instance-size Define the machine size, this is dependent on which provider is chosen. The instance-size correlates with the instance size naming for each cloud. For example, on AWS you would use instance-size = \"c5.4xlarge\" and in Google's case, instance-size = \"e2-standard-4\" . username The ssh user to use when deploying and accessing the nodes. This is also somewhat dependent on which provider is used. In the case of Google , any username can be given. In the case of AWS , the base machines have a pre-loaded user that must be used: AWS Username centos centos debian admin oracle ec2-user sles ec2-user ubuntu ubuntu windows 1 ogc Caution A lot of cloud machine images disable root login, try to avoid using that as a user and utilize sudo for anything requiring elevated permissions. scripts The location on your machine where templates/scripts resides. These will be uploaded and executed during the deployment phase. Note See scripting for more information. scale How many nodes of each layout to deploy. This is also referenced during a deployment reconciliation phase. remote-path (optional) If set, any uploads/downloads outside of what's defined in scripts will be placed in that remote path. include (optional) A list of files/wildcards to include in the upload exclude (optional) A list of files/wildcards to exclude in the upload artifacts (optional) The remote path where script execution output is stored. This is used when pulling artifacts with ogc pull-artifacts and also utilized during node teardown. This will download any artifacts found into artifacts/instance-name/ . tags (optional) Define tags for each layout, allows additional filtering capabilities and deployment options when used with ogc ls and ogc exec ports (optional) Define what ingress ports are available when accessing the node.","title":"Defining Layouts"},{"location":"user-guide/defining-layouts/#variants","text":"OGC supports the concept of variants. In OGC's case, variants are handled by multiple provision spec files and are then merged in a merge-left fashion. What this means is that we can take a base spec file such as: name = \"ci\" [ssh-keys] public = \"id_rsa.pub\" private = \"id_rsa\" [layouts.elastic-agent-sles] runs-on = \"sles-15\" instance-size = \"e2-standard-8\" username = \"ogc\" scripts = \"fixtures/ex_deploy_sles\" provider = \"google\" scale = 5 remote-path = \"/home/ogc/ogc\" include = [ \".ogc-cache\" ] exclude = [ \".git\" , \".venv\" ] artifacts = \"/home/ogc/output/*.xml\" tags = [ \"elastic-agent-8.1.x\" , \"sles-gcp\" ] [layouts.elastic-agent-ubuntu] runs-on = \"ubuntu-latest\" instance-size = \"e2-standard-8\" username = \"ogc\" scripts = \"fixtures/ex_deploy_ubuntu\" provider = \"google\" scale = 5 remote-path = \"/home/ogc/ogc\" exclude = [ \".git\" , \".venv\" ] artifacts = \"/home/ogc/output/*.xml\" tags = [ \"elastic-agent-8.1.x\" , \"ubuntu-gcp\" ] The name of the file doesn't matter, we'll call this file base-spec.toml . Now if we need to change certain aspects of this base deploy specification we can define a second YAML file, we'll call it ubuntu-1804-no-sles.toml . In this example, let's change the username and runs-on for the ubuntu layout, and let's also remove the sles layout: [layouts] elastic-agent-sles = { } [layouts.elastic-agent-ubuntu] runs-on = \"ubuntu-1804\" username = \"ubuntu\" The merging of the specifications will remove any keys that exist if the value of the key is {} (signaling an empty stanza). The remaining keys that match up with the original spec will then be overridden and the rest of the specification is left untouched. Adding new layouts is just a matter of defining another section under layouts in the additional spec files passed to OGC. This is the default user for our contributed packer build for Windows \u21a9","title":"Variants"},{"location":"user-guide/managing-nodes/","text":"Managing a Deployment \u00b6 Learn how to list, inspect, access and debug your node deployments. Listing Nodes \u00b6 To list nodes in your deployment, run the following: $ ogc ls Which gives a table output of current node deployments: You can further drill down with a couple of options: To filter by-tag run: $ ogc ls --by-tag user-adam Accessing nodes \u00b6 OGC provides a helper command for easily accessing any of the nodes in your deployment. To login to one of the above nodes ogc-d7cd61a7-elastic-agent-ubuntu run: $ ogc ssh --by-name ogc-d7cd61a7-elastic-agent-ubuntu ... ssh output ... ogc@ogc-d7cd61a7-elastic-agent-ubuntu:~# Alternatively, use the ID : $ ogc ssh --by-id 38 Executing commands \u00b6 Running arbitrary commands can be accomplished with: $ ogc exec --by-name ogc-d7cd61a7-elastic-agent-ubuntu 'ls -l /' Or if tags are defined, run a command across a set of machines: $ ogc exec --by-tag ubuntu-gcp 'touch this_is_an_ubuntu_machine.txt' Executing a scripts directory \u00b6 In addition to running arbitrary commands, OGC can also execute a directory of templates/scripts: $ ogc exec-scripts --by-name ogc-d7cd61a7-elastic-agent-ubuntu fixtures/ex_deploy_ubuntu Or if tags are defined, run across a set of machines: $ ogc exec-scripts --by-tag ubuntu-gcp fixtures/ex_deploy_ubuntu This can be useful to re-run a deployment or add new functionality/one-offs to a node without disturbing the original layout specifications. Access to the database and all templating is available as well. Downloading files \u00b6 There are 2 ways to download files, the first is to use ogc pull-files , this gives you the ability to download any arbitrary files: $ ogc pull-files ogc-d7cd61a7-elastic-agent-ubuntu im_on_a_computer.txt im_downloaded_computer.txt $ stat im_downloaded_computer.txt 16777221 24809112 -rw-r--r-- 1 adam staff 0 0 \"Mar 24 11:56:24 2022\" \"Mar 24 11:55:16 2022\" \"Mar 24 11:56:24 2022\" \"Mar 24 11:55:16 2022\" 4096 0 0 im_downloaded_computer.txt Another way is if the artifacts key is defined in a layout. To grab files defined by that artifacts option run the following: $ ogc pull-artifacts ogc-d7cd61a7-elastic-agent-ubuntu By default, artifacts are stored in $(pwd)/artifacts/ogc-d7cd61a7-elastic-agent-ubuntu tree artifacts/ogc-d7cd61a7-elastic-agent-ubuntu/ artifacts/ogc-d7cd61a7-elastic-agent-ubuntu/ \u2514\u2500\u2500 test.xml 0 directories, 1 file Uploading files \u00b6 OGC provides a simple way to upload arbitrary files to a node: $ ogc push-files ogc-d7cd61a7-elastic-agent-ubuntu im_downloaded_computer.txt dl.txt Optionally, if --exclude is provided, uploading files will ignore any wildcards matched. Note Passing multiple --exclude is supported and will be added to the list of excludes during upload. Useful if uploading directories and want to ignore things like .git and .venv . Inspecting nodes \u00b6 Each action performed on a node is tracked. This allows you to quickly investigate why scripts failed. To inspect a node and see action results run: $ ogc inspect --id 38 This will return the following output: Deploy Details: ogc-d7cd61a7-elastic-agent-ubuntu [3] Successful Actions: (id: 90) Out: 2022-03-24 12:37:08.657289 '/usr/local/bin/pacman' -> '/usr/local/bin/pacapt' Reading package lists... Building dependency tree... Reading state information... nano is already the newest version (2.9.3-2). nano set to manually installed. The following package was automatically installed and is no longer required: libnuma1 Use 'apt autoremove' to remove it. 0 upgraded, 0 newly installed, 0 to remove and 0 not upgraded. If multiple actions exist, further drill down into the action you want ( seen here \"(id: 90) Out: 2022-03-24 12:37:08.\" in our example ): $ ogc inspect --id 38 --action-id 90 Syncing a deployment \u00b6 In some cases nodes will fail to deploy or you remembered you needed more than 5 nodes or maybe you need less nodes than what the original scale was set. In all these cases, OGC provides a way to keep the deployment in sync with the layouts. To get an idea of the health of the deployment, run: $ ogc status The output returned will be a table displaying what's deployed, the scale, and if there are any remaining nodes left: In cases where you want to add more nodes, update your layout and increase the scale option, in this case we want to add 10 more nodes to our elastic-agent-sles layout: Or another case where we need to reduce the number of nodes from 5 to 3: To perform the sync, run the following: $ ogc status --reconcile And the output will show OGC destroying 2 nodes from each layout: 2022-03-24 at 11:52:37 | INFO Reconciling: [elastic-agent-sles, elastic-agent-ubuntu] 2022-03-24 at 11:52:37 | INFO Destroying: ogc-87ba30fc-elastic-agent-sles 2022-03-24 at 11:52:37 | INFO Destroying: ogc-51b971ad-elastic-agent-sles 2022-03-24 at 11:52:37 | INFO Destroying: ogc-b3befadc-elastic-agent-ubuntu 2022-03-24 at 11:52:37 | INFO Destroying: ogc-d54a5848-elastic-agent-ubuntu Destroying nodes \u00b6 OGC allows destroying of individual or a full blown cleanup. To remove a single node we run: $ ogc rm --by-name ogc-d7cd61a7-elastic-agent-ubuntu --force Or if we wanted to do a full teardown, run: $ ogc rm-all --force","title":"Managing a deployment"},{"location":"user-guide/managing-nodes/#managing-a-deployment","text":"Learn how to list, inspect, access and debug your node deployments.","title":"Managing a Deployment"},{"location":"user-guide/managing-nodes/#listing-nodes","text":"To list nodes in your deployment, run the following: $ ogc ls Which gives a table output of current node deployments: You can further drill down with a couple of options: To filter by-tag run: $ ogc ls --by-tag user-adam","title":"Listing Nodes"},{"location":"user-guide/managing-nodes/#accessing-nodes","text":"OGC provides a helper command for easily accessing any of the nodes in your deployment. To login to one of the above nodes ogc-d7cd61a7-elastic-agent-ubuntu run: $ ogc ssh --by-name ogc-d7cd61a7-elastic-agent-ubuntu ... ssh output ... ogc@ogc-d7cd61a7-elastic-agent-ubuntu:~# Alternatively, use the ID : $ ogc ssh --by-id 38","title":"Accessing nodes"},{"location":"user-guide/managing-nodes/#executing-commands","text":"Running arbitrary commands can be accomplished with: $ ogc exec --by-name ogc-d7cd61a7-elastic-agent-ubuntu 'ls -l /' Or if tags are defined, run a command across a set of machines: $ ogc exec --by-tag ubuntu-gcp 'touch this_is_an_ubuntu_machine.txt'","title":"Executing commands"},{"location":"user-guide/managing-nodes/#executing-a-scripts-directory","text":"In addition to running arbitrary commands, OGC can also execute a directory of templates/scripts: $ ogc exec-scripts --by-name ogc-d7cd61a7-elastic-agent-ubuntu fixtures/ex_deploy_ubuntu Or if tags are defined, run across a set of machines: $ ogc exec-scripts --by-tag ubuntu-gcp fixtures/ex_deploy_ubuntu This can be useful to re-run a deployment or add new functionality/one-offs to a node without disturbing the original layout specifications. Access to the database and all templating is available as well.","title":"Executing a scripts directory"},{"location":"user-guide/managing-nodes/#downloading-files","text":"There are 2 ways to download files, the first is to use ogc pull-files , this gives you the ability to download any arbitrary files: $ ogc pull-files ogc-d7cd61a7-elastic-agent-ubuntu im_on_a_computer.txt im_downloaded_computer.txt $ stat im_downloaded_computer.txt 16777221 24809112 -rw-r--r-- 1 adam staff 0 0 \"Mar 24 11:56:24 2022\" \"Mar 24 11:55:16 2022\" \"Mar 24 11:56:24 2022\" \"Mar 24 11:55:16 2022\" 4096 0 0 im_downloaded_computer.txt Another way is if the artifacts key is defined in a layout. To grab files defined by that artifacts option run the following: $ ogc pull-artifacts ogc-d7cd61a7-elastic-agent-ubuntu By default, artifacts are stored in $(pwd)/artifacts/ogc-d7cd61a7-elastic-agent-ubuntu tree artifacts/ogc-d7cd61a7-elastic-agent-ubuntu/ artifacts/ogc-d7cd61a7-elastic-agent-ubuntu/ \u2514\u2500\u2500 test.xml 0 directories, 1 file","title":"Downloading files"},{"location":"user-guide/managing-nodes/#uploading-files","text":"OGC provides a simple way to upload arbitrary files to a node: $ ogc push-files ogc-d7cd61a7-elastic-agent-ubuntu im_downloaded_computer.txt dl.txt Optionally, if --exclude is provided, uploading files will ignore any wildcards matched. Note Passing multiple --exclude is supported and will be added to the list of excludes during upload. Useful if uploading directories and want to ignore things like .git and .venv .","title":"Uploading files"},{"location":"user-guide/managing-nodes/#inspecting-nodes","text":"Each action performed on a node is tracked. This allows you to quickly investigate why scripts failed. To inspect a node and see action results run: $ ogc inspect --id 38 This will return the following output: Deploy Details: ogc-d7cd61a7-elastic-agent-ubuntu [3] Successful Actions: (id: 90) Out: 2022-03-24 12:37:08.657289 '/usr/local/bin/pacman' -> '/usr/local/bin/pacapt' Reading package lists... Building dependency tree... Reading state information... nano is already the newest version (2.9.3-2). nano set to manually installed. The following package was automatically installed and is no longer required: libnuma1 Use 'apt autoremove' to remove it. 0 upgraded, 0 newly installed, 0 to remove and 0 not upgraded. If multiple actions exist, further drill down into the action you want ( seen here \"(id: 90) Out: 2022-03-24 12:37:08.\" in our example ): $ ogc inspect --id 38 --action-id 90","title":"Inspecting nodes"},{"location":"user-guide/managing-nodes/#syncing-a-deployment","text":"In some cases nodes will fail to deploy or you remembered you needed more than 5 nodes or maybe you need less nodes than what the original scale was set. In all these cases, OGC provides a way to keep the deployment in sync with the layouts. To get an idea of the health of the deployment, run: $ ogc status The output returned will be a table displaying what's deployed, the scale, and if there are any remaining nodes left: In cases where you want to add more nodes, update your layout and increase the scale option, in this case we want to add 10 more nodes to our elastic-agent-sles layout: Or another case where we need to reduce the number of nodes from 5 to 3: To perform the sync, run the following: $ ogc status --reconcile And the output will show OGC destroying 2 nodes from each layout: 2022-03-24 at 11:52:37 | INFO Reconciling: [elastic-agent-sles, elastic-agent-ubuntu] 2022-03-24 at 11:52:37 | INFO Destroying: ogc-87ba30fc-elastic-agent-sles 2022-03-24 at 11:52:37 | INFO Destroying: ogc-51b971ad-elastic-agent-sles 2022-03-24 at 11:52:37 | INFO Destroying: ogc-b3befadc-elastic-agent-ubuntu 2022-03-24 at 11:52:37 | INFO Destroying: ogc-d54a5848-elastic-agent-ubuntu","title":"Syncing a deployment"},{"location":"user-guide/managing-nodes/#destroying-nodes","text":"OGC allows destroying of individual or a full blown cleanup. To remove a single node we run: $ ogc rm --by-name ogc-d7cd61a7-elastic-agent-ubuntu --force Or if we wanted to do a full teardown, run: $ ogc rm-all --force","title":"Destroying nodes"},{"location":"user-guide/providers/","text":"Providers \u00b6 In order to access a cloud provider, there are certain environment variables that need to be exposed for each. Each environment variable should be defined in .env file so it will be automatically loaded when running OGC. AWS \u00b6 AWS_ACCESS_KEY_ID AWS_SECRET_ACCESS_KEY AWS_REGION Google \u00b6 GOOGLE_APPLICATION_CREDENTIALS GOOGLE_APPLICATION_SERVICE_ACCOUNT GOOGLE_PROJECT GOOGLE_DATACENTER","title":"Providers"},{"location":"user-guide/providers/#providers","text":"In order to access a cloud provider, there are certain environment variables that need to be exposed for each. Each environment variable should be defined in .env file so it will be automatically loaded when running OGC.","title":"Providers"},{"location":"user-guide/providers/#aws","text":"AWS_ACCESS_KEY_ID AWS_SECRET_ACCESS_KEY AWS_REGION","title":"AWS"},{"location":"user-guide/providers/#google","text":"GOOGLE_APPLICATION_CREDENTIALS GOOGLE_APPLICATION_SERVICE_ACCOUNT GOOGLE_PROJECT GOOGLE_DATACENTER","title":"Google"},{"location":"user-guide/scripting/","text":"Scripting \u00b6 All deployments have the ability to execute scripts once a node becomes available. Before starting \u00b6 A couple of things to keep in mind: All scripts are executed in order based on the filenames. It is recommended to create scripts with a numbered prefix, for example: - scripts/ - 01-install-deps - 02-configure-services There is a special reserved filename teardown , if this file exists it will only be executed during a removal of a node. This is useful for any cleanup actions that may need to be run, such as removing test users, un-enrolling from a service, etc. Writing scripts \u00b6 Scripts can be written in any language, it is up to you to configure the nodes so that any runtimes and library dependencies are met on the target node for your script to execute in. One way to accomplish this is to create 01-setup-env bash script: #!/bin/bash echo \"Installing python3 on ubuntu\" sudo apt-get update sudo apt-get install -qyf python3 sudo pip install sh Then in subsequent scripts, using python3 is available. For example, in file 02-run-cmd-in-python : #!/usr/bin/env python3 import sh sh . ls ( '/' ) sh . cp ( '-a' , 'mydir' , 'anotherdir' ) Templating \u00b6 OGC provides some additional capabilities through templating. Under the hood python-mako is used for the parsing. With templating, you have the ability to query the underlying database to gather node information, a couple of modules are already exposed in the templates context: Var Description db Exposes access to the database node Current deployed node metadata env Environment variables are made available through this key, env['USER'] user Current user metadata #!/bin/bash echo \"\" % for node in db.get_nodes () .unwrap () : echo \"[ID: ${ node .instance_id } ] Name: ${ node .instance_name } || Connection: ${ node .username } @ ${ node .public_ip } || Provider: ${ node .layout.provider } \" % endfor echo \"\" The runtime environment is also available within the template context. In one example, we can export the following into our .env file and reference those in the templates: OGC_ELASTIC_AGENT_VERSION OGC_ELASTIC_AGENT_SHA OGC_ELASTIC_AGENT_VERSION OGC_FLEET_URL OGC_FLEET_ENROLLMENT_TOKEN See the below example for downloading elastic-agent and enrolling it into a fleet server: #!/bin/bash <%namespace name = \"utils\" file = \"/functions.mako\" /> <% url = \"https://staging.elastic.co/%s-%s/downloads/beats/elastic-agent/elastic-agent-%s-linux-x86_64.tar.gz\" % ( env [ 'OGC_ELASTIC_AGENT_VERSION' ] , env [ 'OGC_ELASTIC_AGENT_SHA' ] , env [ 'OGC_ELASTIC_AGENT_VERSION' ]) %> ${ utils .setup_env() } ${ utils .install_pkgs([ 'nano' ]) } ${ utils .download(url, 'elastic-agent.tar.gz' ) } ${ utils .extract( 'elastic-agent.tar.gz' ) } mv elastic-agent- ${ env [ 'OGC_ELASTIC_AGENT_VERSION' ] } -linux-x86_64 elastic-agent cd elastic-agent && ./elastic-agent install -f --url = ${ env [ 'OGC_FLEET_URL' ] } --enrollment-token = ${ env [ 'OGC_FLEET_ENROLLMENT_TOKEN' ] } Reusable helpers \u00b6 In the above example we reference a file called /functions.mako this is just another template file that sits just outside of our defined scripts , for example, if our scripts is defined to be in scripts/my_ubuntu_deploy then this functions.mako will live at scripts/functions.mako . Alert This is good practice as you may have multiple layouts with different script directories for each and would like to store common functionality in a single place. Defining helper functions is straight forward, lets look at functions.mako for an example: ## Helper template functions downloading/extracting files <%def name = \"setup_env()\" > if ! test -f \"/usr/local/bin/pacapt\" ; then wget -O /usr/local/bin/pacapt https://github.com/icy/pacapt/raw/ng/pacapt chmod 755 /usr/local/bin/pacapt ln -sv /usr/local/bin/pacapt /usr/local/bin/pacman || true fi </%def> <%def name = \"install_pkgs(pkgs)\" > % for pkg in pkgs: pacapt install --noconfirm ${ pkg } % endfor </%def> <%def name = \"download(url, src_file)\" > wget -O ${ src_file } ${ url } </%def> <%def name = \"extract(src, dst=None)\" > % if dst: mkdir -p ${ dst } tar -xvf ${ src } -C ${ dst } % else : tar -xvf ${ src } % endif </%def> Each %def section defines a function block that when called with any necessary arguments will output that data into the scripts with all necessary translations handled. You can see the usage of these functions in the previous example for installing elastic-agent. It is worth the time to visit Mako's website and learn about its feature set, particularly namespaces and defs and blocks .","title":"Scripting"},{"location":"user-guide/scripting/#scripting","text":"All deployments have the ability to execute scripts once a node becomes available.","title":"Scripting"},{"location":"user-guide/scripting/#before-starting","text":"A couple of things to keep in mind: All scripts are executed in order based on the filenames. It is recommended to create scripts with a numbered prefix, for example: - scripts/ - 01-install-deps - 02-configure-services There is a special reserved filename teardown , if this file exists it will only be executed during a removal of a node. This is useful for any cleanup actions that may need to be run, such as removing test users, un-enrolling from a service, etc.","title":"Before starting"},{"location":"user-guide/scripting/#writing-scripts","text":"Scripts can be written in any language, it is up to you to configure the nodes so that any runtimes and library dependencies are met on the target node for your script to execute in. One way to accomplish this is to create 01-setup-env bash script: #!/bin/bash echo \"Installing python3 on ubuntu\" sudo apt-get update sudo apt-get install -qyf python3 sudo pip install sh Then in subsequent scripts, using python3 is available. For example, in file 02-run-cmd-in-python : #!/usr/bin/env python3 import sh sh . ls ( '/' ) sh . cp ( '-a' , 'mydir' , 'anotherdir' )","title":"Writing scripts"},{"location":"user-guide/scripting/#templating","text":"OGC provides some additional capabilities through templating. Under the hood python-mako is used for the parsing. With templating, you have the ability to query the underlying database to gather node information, a couple of modules are already exposed in the templates context: Var Description db Exposes access to the database node Current deployed node metadata env Environment variables are made available through this key, env['USER'] user Current user metadata #!/bin/bash echo \"\" % for node in db.get_nodes () .unwrap () : echo \"[ID: ${ node .instance_id } ] Name: ${ node .instance_name } || Connection: ${ node .username } @ ${ node .public_ip } || Provider: ${ node .layout.provider } \" % endfor echo \"\" The runtime environment is also available within the template context. In one example, we can export the following into our .env file and reference those in the templates: OGC_ELASTIC_AGENT_VERSION OGC_ELASTIC_AGENT_SHA OGC_ELASTIC_AGENT_VERSION OGC_FLEET_URL OGC_FLEET_ENROLLMENT_TOKEN See the below example for downloading elastic-agent and enrolling it into a fleet server: #!/bin/bash <%namespace name = \"utils\" file = \"/functions.mako\" /> <% url = \"https://staging.elastic.co/%s-%s/downloads/beats/elastic-agent/elastic-agent-%s-linux-x86_64.tar.gz\" % ( env [ 'OGC_ELASTIC_AGENT_VERSION' ] , env [ 'OGC_ELASTIC_AGENT_SHA' ] , env [ 'OGC_ELASTIC_AGENT_VERSION' ]) %> ${ utils .setup_env() } ${ utils .install_pkgs([ 'nano' ]) } ${ utils .download(url, 'elastic-agent.tar.gz' ) } ${ utils .extract( 'elastic-agent.tar.gz' ) } mv elastic-agent- ${ env [ 'OGC_ELASTIC_AGENT_VERSION' ] } -linux-x86_64 elastic-agent cd elastic-agent && ./elastic-agent install -f --url = ${ env [ 'OGC_FLEET_URL' ] } --enrollment-token = ${ env [ 'OGC_FLEET_ENROLLMENT_TOKEN' ] }","title":"Templating"},{"location":"user-guide/scripting/#reusable-helpers","text":"In the above example we reference a file called /functions.mako this is just another template file that sits just outside of our defined scripts , for example, if our scripts is defined to be in scripts/my_ubuntu_deploy then this functions.mako will live at scripts/functions.mako . Alert This is good practice as you may have multiple layouts with different script directories for each and would like to store common functionality in a single place. Defining helper functions is straight forward, lets look at functions.mako for an example: ## Helper template functions downloading/extracting files <%def name = \"setup_env()\" > if ! test -f \"/usr/local/bin/pacapt\" ; then wget -O /usr/local/bin/pacapt https://github.com/icy/pacapt/raw/ng/pacapt chmod 755 /usr/local/bin/pacapt ln -sv /usr/local/bin/pacapt /usr/local/bin/pacman || true fi </%def> <%def name = \"install_pkgs(pkgs)\" > % for pkg in pkgs: pacapt install --noconfirm ${ pkg } % endfor </%def> <%def name = \"download(url, src_file)\" > wget -O ${ src_file } ${ url } </%def> <%def name = \"extract(src, dst=None)\" > % if dst: mkdir -p ${ dst } tar -xvf ${ src } -C ${ dst } % else : tar -xvf ${ src } % endif </%def> Each %def section defines a function block that when called with any necessary arguments will output that data into the scripts with all necessary translations handled. You can see the usage of these functions in the previous example for installing elastic-agent. It is worth the time to visit Mako's website and learn about its feature set, particularly namespaces and defs and blocks .","title":"Reusable helpers"},{"location":"user-guide/sharing/","text":"Sharing Environment \u00b6 OGC provides a way to share an environment with another user, they are facilitated through the ogc export-env and ogc import-env commands. This document will walk through how to accomplish this plus any caveats to be aware of. Exporting \u00b6 First, we need to export the current environment. This will dump the database of node information, the environment variables used in the deployment, and will include the public ssh key of the shared user on all nodes. The person you are sharing with should provide you with a passwordless public ssh key. This can either be the contents of the public ssh key or a GitHub username that has public ssh keys associated with their profile. $ ogc export-env How would you like to import the users public ssh key [ github/manual ] ( github ) : Please enter your Github username: adam-stokes Once the export is complete it will print out the results including some instructions on what to do next Importing \u00b6 Once the user has both the database ogc-dump.sql and the environment ogc-env.json they will need to import that on their machine. Caution If the user already has ogc installed and a deployment created, they may want to keep that information intact. To avoid overwriting the users database during import make sure to prefix the command with POSTGRES_DB : $ POSTGRES_DB = \"ogcopydb\" ogc import-env Also, the import command will automatically write to files ogc.yml and .env . If those files already exist in the directory where the import command is run then you will want to do one of two things: Create a backup of those files Run ogc import-env from a different directory, perhaps creating a temporary directory for working with the shared environment. To do the import run the following: $ POSTGRES_DB = \"ogcopydb\" ogc import-env --env-file ../ogc-env.json \\ --db-file ../ogc-dump.sql \\ --private-ssh-key ~/.ssh/id_rsa \\ --public-ssh-key ~/.ssh/id_rsa.pub The private-ssh-key and public-ssh-key should be the locations of the keys associated with the one shared during export. Once import is complete, you can access this environment: $ POSTGRES_DB = \"ogcopydb\" ogc ls Warning The cloud credentials are not copied over, you will need to setup your credentials again in the newly created .env . You will need to make sure that the correct project/region is set and accessible by your cloud account.","title":"Sharing Environment"},{"location":"user-guide/sharing/#sharing-environment","text":"OGC provides a way to share an environment with another user, they are facilitated through the ogc export-env and ogc import-env commands. This document will walk through how to accomplish this plus any caveats to be aware of.","title":"Sharing Environment"},{"location":"user-guide/sharing/#exporting","text":"First, we need to export the current environment. This will dump the database of node information, the environment variables used in the deployment, and will include the public ssh key of the shared user on all nodes. The person you are sharing with should provide you with a passwordless public ssh key. This can either be the contents of the public ssh key or a GitHub username that has public ssh keys associated with their profile. $ ogc export-env How would you like to import the users public ssh key [ github/manual ] ( github ) : Please enter your Github username: adam-stokes Once the export is complete it will print out the results including some instructions on what to do next","title":"Exporting"},{"location":"user-guide/sharing/#importing","text":"Once the user has both the database ogc-dump.sql and the environment ogc-env.json they will need to import that on their machine. Caution If the user already has ogc installed and a deployment created, they may want to keep that information intact. To avoid overwriting the users database during import make sure to prefix the command with POSTGRES_DB : $ POSTGRES_DB = \"ogcopydb\" ogc import-env Also, the import command will automatically write to files ogc.yml and .env . If those files already exist in the directory where the import command is run then you will want to do one of two things: Create a backup of those files Run ogc import-env from a different directory, perhaps creating a temporary directory for working with the shared environment. To do the import run the following: $ POSTGRES_DB = \"ogcopydb\" ogc import-env --env-file ../ogc-env.json \\ --db-file ../ogc-dump.sql \\ --private-ssh-key ~/.ssh/id_rsa \\ --public-ssh-key ~/.ssh/id_rsa.pub The private-ssh-key and public-ssh-key should be the locations of the keys associated with the one shared during export. Once import is complete, you can access this environment: $ POSTGRES_DB = \"ogcopydb\" ogc ls Warning The cloud credentials are not copied over, you will need to setup your credentials again in the newly created .env . You will need to make sure that the correct project/region is set and accessible by your cloud account.","title":"Importing"},{"location":"user-guide/windows/","text":"Windows \u00b6 OGC supports provisioning Windows instances, however, it does make a couple of assumptions: OpenSSH Server is running on the Windows Machine Rsync is installed and available Passwordless ssh is setup Fortunately, we provide you with a Packer setup that will let you quickly build an AWS AMI to meet those requirements. Warning If using OGC contributed packer build, only AWS is supported at this time. Build AMI \u00b6 The configurations are located in contrib/ , to get started run: $ git clone https://github.com/adam-stokes/ogc $ cd ogc/contrib Alert If using these Packer configs, please note the default user to use is: ogc Windows 2019 \u00b6 To build a Windows 2019 Server instance run: ogc/contrib> $ packer build windows2019.json Once complete, grab the AMI ID, as this will be used in the layout specification of OGC. Usage \u00b6 To provision and deploy a Windows machine, the following example spec will work: name = \"ci\" [ssh-keys] public = \"~/.ssh/id_rsa_libcloud.pub\" private = \"~/.ssh/id_rsa_libcloud\" [layouts.elastic-agent-windows] runs-on = \"ami-0587bd602f1da2f1d\" instance-size = \"c5.2xlarge\" username = \"ogc\" scripts = \"fixtures/ex_deploy_windows\" provider = \"aws\" scale = 1 remote-path = \"ogc-src\" exclude = [ \".git\" , \".venv\" , \"artifacts\" ] artifacts = \"output\\\\*.xml\" tags = [ \"elastic-agent-8.1.x\" , \"windows-aws\" ] ports = [ \"22:22\" ] Once defined, simply running: $ ogc launch Will get you a provisioned Windows machine! Scripting \u00b6 Powershell a good choice, works out of the box on Windows, however, if you want to use a different programming language the choice is yours. All the templating, database, and context is available. For example, to print out the current node information, edit a file 01-powershell : powershell echo \"${node.instance_name}:${node.public_ip}\" > ${node.instance_name}.txt This is a simple example, for a more advanced deployment it may be best to create your ps1 files and then reference them through powershell interpreter.","title":"Windows"},{"location":"user-guide/windows/#windows","text":"OGC supports provisioning Windows instances, however, it does make a couple of assumptions: OpenSSH Server is running on the Windows Machine Rsync is installed and available Passwordless ssh is setup Fortunately, we provide you with a Packer setup that will let you quickly build an AWS AMI to meet those requirements. Warning If using OGC contributed packer build, only AWS is supported at this time.","title":"Windows"},{"location":"user-guide/windows/#build-ami","text":"The configurations are located in contrib/ , to get started run: $ git clone https://github.com/adam-stokes/ogc $ cd ogc/contrib Alert If using these Packer configs, please note the default user to use is: ogc","title":"Build AMI"},{"location":"user-guide/windows/#windows-2019","text":"To build a Windows 2019 Server instance run: ogc/contrib> $ packer build windows2019.json Once complete, grab the AMI ID, as this will be used in the layout specification of OGC.","title":"Windows 2019"},{"location":"user-guide/windows/#usage","text":"To provision and deploy a Windows machine, the following example spec will work: name = \"ci\" [ssh-keys] public = \"~/.ssh/id_rsa_libcloud.pub\" private = \"~/.ssh/id_rsa_libcloud\" [layouts.elastic-agent-windows] runs-on = \"ami-0587bd602f1da2f1d\" instance-size = \"c5.2xlarge\" username = \"ogc\" scripts = \"fixtures/ex_deploy_windows\" provider = \"aws\" scale = 1 remote-path = \"ogc-src\" exclude = [ \".git\" , \".venv\" , \"artifacts\" ] artifacts = \"output\\\\*.xml\" tags = [ \"elastic-agent-8.1.x\" , \"windows-aws\" ] ports = [ \"22:22\" ] Once defined, simply running: $ ogc launch Will get you a provisioned Windows machine!","title":"Usage"},{"location":"user-guide/windows/#scripting","text":"Powershell a good choice, works out of the box on Windows, however, if you want to use a different programming language the choice is yours. All the templating, database, and context is available. For example, to print out the current node information, edit a file 01-powershell : powershell echo \"${node.instance_name}:${node.public_ip}\" > ${node.instance_name}.txt This is a simple example, for a more advanced deployment it may be best to create your ps1 files and then reference them through powershell interpreter.","title":"Scripting"},{"location":"user-guide/cookbook/template-access-node-info/","text":"Accessing node information \u00b6 Current node \u00b6 In template files you have access to the node that is currently being rendered prior to those scripts being uploaded. Below is an example of querying the current node's information and outputting it to a file: #!/bin/bash echo \"### CURRENT NODE\" >> node_info.txt echo \"[ID: ${ node .id } ] Name: ${ node .instance_name } || Connection: ${ node .username } @ ${ node .public_ip } || Provider: ${ node .provider } \" >> node_info.txt echo \"### CURRENT NODE\" >> node_info.txt Save this file in the location of your defined scripts and give it a indexed name of where in the order it should be executed, for example, 01-show-node-info 1 . All nodes \u00b6 In some cases you may need to grab information from another node in the deployment, for example, a second node running Kibana in which the first node needs to perform some kind of API calls against it. We can accomplish this using the db and session modules that's exposed in our templates. Create a file 02-curl-remote with the following: #!/bin/bash sudo pip install httpie KIBANA_HOST = ${ session .query(db.Node).filter(db.Node.instance_name.contains([ \"kibana\" ]).first() or '' ) } http -a username:passsword -f GET https:// $KIBANA_HOST :5601/fleet/setup kbn-xsrf:ogc See the Scripting documentation for ordering of files. \u21a9","title":"Access node info"},{"location":"user-guide/cookbook/template-access-node-info/#accessing-node-information","text":"","title":"Accessing node information"},{"location":"user-guide/cookbook/template-access-node-info/#current-node","text":"In template files you have access to the node that is currently being rendered prior to those scripts being uploaded. Below is an example of querying the current node's information and outputting it to a file: #!/bin/bash echo \"### CURRENT NODE\" >> node_info.txt echo \"[ID: ${ node .id } ] Name: ${ node .instance_name } || Connection: ${ node .username } @ ${ node .public_ip } || Provider: ${ node .provider } \" >> node_info.txt echo \"### CURRENT NODE\" >> node_info.txt Save this file in the location of your defined scripts and give it a indexed name of where in the order it should be executed, for example, 01-show-node-info 1 .","title":"Current node"},{"location":"user-guide/cookbook/template-access-node-info/#all-nodes","text":"In some cases you may need to grab information from another node in the deployment, for example, a second node running Kibana in which the first node needs to perform some kind of API calls against it. We can accomplish this using the db and session modules that's exposed in our templates. Create a file 02-curl-remote with the following: #!/bin/bash sudo pip install httpie KIBANA_HOST = ${ session .query(db.Node).filter(db.Node.instance_name.contains([ \"kibana\" ]).first() or '' ) } http -a username:passsword -f GET https:// $KIBANA_HOST :5601/fleet/setup kbn-xsrf:ogc See the Scripting documentation for ordering of files. \u21a9","title":"All nodes"}]}