{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"OGC \u00b6 ogc - provisioning, that's it. Getting Started \u00b6 Welcome to the getting started guide! This should be a quick introduction to get up and running with OGC. More information on customizing and extending OGC can be found in the user documentation. Setup \u00b6 OGC requires Postgres to function. The easiest way to fulfill this requirement is with docker-compose : version : \"3.9\" services : postgres : image : postgres:11 environment : - POSTGRES_DB=ogc - POSTGRES_USER=postgres - POSTGRES_PASSWORD=postgres healthcheck : test : [ \"CMD\" , \"pg_isready\" , \"-U\" , \"postgres\" ] retries : 300 interval : 1s ports : - '5432:5432' Bring up the services $ docker-compose up Info To connect to a remote postgres database export the following environment variables POSTGRES_HOST POSTGRES_PORT POSTGRES_DB POSTGRES_USER POSTGRES_PASSWORD Next , is installation of OGC. We use Poetry : $ pip install poetry $ poetry install Caution If using poetry make sure to prefix running of ogc with the following: $ poetry run ogc Optionally, load up the virtualenv beforehand: $ poetry shell Or install from pypi : $ pip install ogc Initialize \u00b6 Next is to initialize the OGC environment, to do that run: $ ogc init It will ask you for a name, feel free to put something other than your actual name if desired. Provider Setup \u00b6 OGC currently supports AWS and GCP out of the box (more added soon). In order for OGC to connect and deploy to these clouds a few environment variables are needed. Create a .env file in the top level directory where ogc is to be run: AWS_ACCESS_KEY_ID=\"\" AWS_SECRET_ACCESS_KEY=\"\" AWS_REGION=\"us-east-2\" GOOGLE_APPLICATION_CREDENTIALS=\"svc.json\" GOOGLE_APPLICATION_SERVICE_ACCOUNT=\"..@...iam.gserviceaccount.com\" GOOGLE_PROJECT=\"example-project\" GOOGLE_DATACENTER=\"us-central1-a\" Note More information can be found in our Providers documentation. Define Provisioning \u00b6 Once setup is complete, a provision specification is needed. This defines ssh-keys and one or more layouts to be provisioned. Create a file ogc.yml and place in the top level directory where ogc is run: name : ci # SSH Keys must be passwordless ssh-keys : public : ~/.ssh/id_rsa_libcloud.pub private : ~/.ssh/id_rsa_libcloud layouts : elastic-agent-sles : runs-on : sles-15 instance-size : e2-standard-8 username : ogc scripts : fixtures/ex_deploy_sles provider : google scale : 5 remote-path : /home/ogc/ogc include : - .ogc-cache exclude : - .git - .venv artifacts : /home/ogc/output/*.xml tags : - elastic-agent-8.1.x - sles-gcp This specification tells OGC to deploy 5 nodes running on Google's e2-standard-8 with SUSE 15 OS. The scripts section tells OGC where the template files/scripts are located that need to be uploaded to each node during the deployment phase. Provision and Deploy \u00b6 Once the specification is set, environment variables configured and a postgres database is accessible, execute a deployment in a new terminal: $ ogc launch Note If the file is something other than ogc.yml append the --spec option to the launch command: $ ogc launch --spec my-custom-provision.yml Next steps \u00b6 Learn how to manage your deployments in our User Guide - Managing a deployment","title":"Getting Started"},{"location":"#ogc","text":"ogc - provisioning, that's it.","title":"OGC"},{"location":"#getting-started","text":"Welcome to the getting started guide! This should be a quick introduction to get up and running with OGC. More information on customizing and extending OGC can be found in the user documentation.","title":"Getting Started"},{"location":"#setup","text":"OGC requires Postgres to function. The easiest way to fulfill this requirement is with docker-compose : version : \"3.9\" services : postgres : image : postgres:11 environment : - POSTGRES_DB=ogc - POSTGRES_USER=postgres - POSTGRES_PASSWORD=postgres healthcheck : test : [ \"CMD\" , \"pg_isready\" , \"-U\" , \"postgres\" ] retries : 300 interval : 1s ports : - '5432:5432' Bring up the services $ docker-compose up Info To connect to a remote postgres database export the following environment variables POSTGRES_HOST POSTGRES_PORT POSTGRES_DB POSTGRES_USER POSTGRES_PASSWORD Next , is installation of OGC. We use Poetry : $ pip install poetry $ poetry install Caution If using poetry make sure to prefix running of ogc with the following: $ poetry run ogc Optionally, load up the virtualenv beforehand: $ poetry shell Or install from pypi : $ pip install ogc","title":"Setup"},{"location":"#initialize","text":"Next is to initialize the OGC environment, to do that run: $ ogc init It will ask you for a name, feel free to put something other than your actual name if desired.","title":"Initialize"},{"location":"#provider-setup","text":"OGC currently supports AWS and GCP out of the box (more added soon). In order for OGC to connect and deploy to these clouds a few environment variables are needed. Create a .env file in the top level directory where ogc is to be run: AWS_ACCESS_KEY_ID=\"\" AWS_SECRET_ACCESS_KEY=\"\" AWS_REGION=\"us-east-2\" GOOGLE_APPLICATION_CREDENTIALS=\"svc.json\" GOOGLE_APPLICATION_SERVICE_ACCOUNT=\"..@...iam.gserviceaccount.com\" GOOGLE_PROJECT=\"example-project\" GOOGLE_DATACENTER=\"us-central1-a\" Note More information can be found in our Providers documentation.","title":"Provider Setup"},{"location":"#define-provisioning","text":"Once setup is complete, a provision specification is needed. This defines ssh-keys and one or more layouts to be provisioned. Create a file ogc.yml and place in the top level directory where ogc is run: name : ci # SSH Keys must be passwordless ssh-keys : public : ~/.ssh/id_rsa_libcloud.pub private : ~/.ssh/id_rsa_libcloud layouts : elastic-agent-sles : runs-on : sles-15 instance-size : e2-standard-8 username : ogc scripts : fixtures/ex_deploy_sles provider : google scale : 5 remote-path : /home/ogc/ogc include : - .ogc-cache exclude : - .git - .venv artifacts : /home/ogc/output/*.xml tags : - elastic-agent-8.1.x - sles-gcp This specification tells OGC to deploy 5 nodes running on Google's e2-standard-8 with SUSE 15 OS. The scripts section tells OGC where the template files/scripts are located that need to be uploaded to each node during the deployment phase.","title":"Define Provisioning"},{"location":"#provision-and-deploy","text":"Once the specification is set, environment variables configured and a postgres database is accessible, execute a deployment in a new terminal: $ ogc launch Note If the file is something other than ogc.yml append the --spec option to the launch command: $ ogc launch --spec my-custom-provision.yml","title":"Provision and Deploy"},{"location":"#next-steps","text":"Learn how to manage your deployments in our User Guide - Managing a deployment","title":"Next steps"},{"location":"commands/ogc/","text":"CLI Reference \u00b6 This page provides documentation for our command line tool. ogc \u00b6 Just a simple provisioner Usage: ogc [OPTIONS] COMMAND [ARGS]... Options: Name Type Description Default --help boolean Show this message and exit. False ogc db-migrate \u00b6 Database migrations Usage: ogc db-migrate [OPTIONS] Options: Name Type Description Default --help boolean Show this message and exit. False ogc exec \u00b6 Execute a command across node(s) Usage: ogc exec [OPTIONS] CMD Options: Name Type Description Default --by-tag text Only run on nodes matching tag None --by-name text Only run on nodes matching name None --help boolean Show this message and exit. False ogc exec-scripts \u00b6 (R)Execute a set of scripts Usage: ogc exec-scripts [OPTIONS] PATH Options: Name Type Description Default --by-tag text Only run on nodes matching tag None --by-name text Only run on nodes matching name None --help boolean Show this message and exit. False ogc export-env \u00b6 Exports the deployment to be shared with other users This exports the current database and imports the public ssh key of the shared user. Note on cloud credentials: They are not exported and must be set on the shared users environment. Usage: ogc export-env [OPTIONS] Options: Name Type Description Default --spec text Location of the ogc.yml or other spec files to include in export None --db-file text Filename of the database dump ogc-dump.sql --env-file text Filename of where to store the OGC environment to be shared ogc-env.json --help boolean Show this message and exit. False ogc import-env \u00b6 Imports a shared deployment Usage: ogc import-env [OPTIONS] Options: Name Type Description Default --db-file text Filename of the database dump ogc-dump.sql --env-file text Filename of where to store the OGC environment to be shared ogc-env.json --private-ssh-key text The path to your private ssh key. This must match the public ssh key used during the export. _required --public-ssh-key text The path to your public ssh key. This must match the ssh key used during the export. _required --help boolean Show this message and exit. False ogc init \u00b6 Initialize OGC Usage: ogc init [OPTIONS] Options: Name Type Description Default --help boolean Show this message and exit. False ogc inspect \u00b6 List nodes in your inventory Usage: ogc inspect [OPTIONS] Options: Name Type Description Default --id text Inspect node by DB ID None --name text Inspect nodes by name, this can be a substring match None --tag text Inspect nodes by tag None --action-id text If set will only show the action output for a specific action ID None --extend / --no-extend boolean Show extended action output at once False --help boolean Show this message and exit. False ogc launch \u00b6 Launches nodes from a provision specification Usage: ogc launch [OPTIONS] Options: Name Type Description Default --spec text N/A None --with-deploy / --with-no-deploy boolean Also performs script deployments True --help boolean Show this message and exit. False ogc ls \u00b6 List nodes in your inventory Usage: ogc ls [OPTIONS] Options: Name Type Description Default --by-tag text List nodes by tag None --by-name text List nodes by name, this can be a substring match None --output-file text Stores the table output to svg or html. Determined by the file extension. None --help boolean Show this message and exit. False ogc ls-key-pairs \u00b6 List keypairs Usage: ogc ls-key-pairs [OPTIONS] Options: Name Type Description Default --filter text Filter by keypair name None --provider text Provider to query aws --help boolean Show this message and exit. False ogc pull-artifacts \u00b6 Download artifacts from node Usage: ogc pull-artifacts [OPTIONS] NAME Options: Name Type Description Default --help boolean Show this message and exit. False ogc pull-files \u00b6 Scp files or directories from node Usage: ogc pull-files [OPTIONS] NAME DST SRC Options: Name Type Description Default --help boolean Show this message and exit. False ogc push-files \u00b6 Scp files or directories to node Usage: ogc push-files [OPTIONS] NAME SRC DST Options: Name Type Description Default --exclude text Exclude files/directories when uploading None --help boolean Show this message and exit. False ogc rm \u00b6 Destroys a node and its associated keys, storage, etc. Usage: ogc rm [OPTIONS] Options: Name Type Description Default --by-name text Remove node by its Name None --force / --no-force boolean Force removal regardless of connectivity False --only-db / --no-only-db boolean Force removal of database records only False --help boolean Show this message and exit. False ogc rm-all \u00b6 Destroys everything. Use with caution. Usage: ogc rm-all [OPTIONS] Options: Name Type Description Default --force / --no-force boolean Force removal regardless of connectivity False --only-db / --no-only-db boolean Force removal of database records only False --help boolean Show this message and exit. False ogc rm-key-pairs \u00b6 Remove keypairs Usage: ogc rm-key-pairs [OPTIONS] Options: Name Type Description Default --filter text Filter by keypair name None --provider text Provider to query aws --help boolean Show this message and exit. False ogc shell \u00b6 Launches IPython REPL Usage: ogc shell [OPTIONS] Options: Name Type Description Default --help boolean Show this message and exit. False ogc ssh \u00b6 Login to a node Usage: ogc ssh [OPTIONS] Options: Name Type Description Default --by-id text Login to a node by its ID None --by-name text Login to a node by its Name None --help boolean Show this message and exit. False ogc status \u00b6 Get status of deployment Usage: ogc status [OPTIONS] Options: Name Type Description Default --reconcile / --no-reconcile boolean Attempt to fix deployment to match scale False --spec text N/A None --output-file text Stores the table output to svg or html. Determined by the file extension. None --help boolean Show this message and exit. False","title":"CLI Reference"},{"location":"commands/ogc/#cli-reference","text":"This page provides documentation for our command line tool.","title":"CLI Reference"},{"location":"commands/ogc/#ogc","text":"Just a simple provisioner Usage: ogc [OPTIONS] COMMAND [ARGS]... Options: Name Type Description Default --help boolean Show this message and exit. False","title":"ogc"},{"location":"commands/ogc/#ogc-db-migrate","text":"Database migrations Usage: ogc db-migrate [OPTIONS] Options: Name Type Description Default --help boolean Show this message and exit. False","title":"db-migrate"},{"location":"commands/ogc/#ogc-exec","text":"Execute a command across node(s) Usage: ogc exec [OPTIONS] CMD Options: Name Type Description Default --by-tag text Only run on nodes matching tag None --by-name text Only run on nodes matching name None --help boolean Show this message and exit. False","title":"exec"},{"location":"commands/ogc/#ogc-exec-scripts","text":"(R)Execute a set of scripts Usage: ogc exec-scripts [OPTIONS] PATH Options: Name Type Description Default --by-tag text Only run on nodes matching tag None --by-name text Only run on nodes matching name None --help boolean Show this message and exit. False","title":"exec-scripts"},{"location":"commands/ogc/#ogc-export-env","text":"Exports the deployment to be shared with other users This exports the current database and imports the public ssh key of the shared user. Note on cloud credentials: They are not exported and must be set on the shared users environment. Usage: ogc export-env [OPTIONS] Options: Name Type Description Default --spec text Location of the ogc.yml or other spec files to include in export None --db-file text Filename of the database dump ogc-dump.sql --env-file text Filename of where to store the OGC environment to be shared ogc-env.json --help boolean Show this message and exit. False","title":"export-env"},{"location":"commands/ogc/#ogc-import-env","text":"Imports a shared deployment Usage: ogc import-env [OPTIONS] Options: Name Type Description Default --db-file text Filename of the database dump ogc-dump.sql --env-file text Filename of where to store the OGC environment to be shared ogc-env.json --private-ssh-key text The path to your private ssh key. This must match the public ssh key used during the export. _required --public-ssh-key text The path to your public ssh key. This must match the ssh key used during the export. _required --help boolean Show this message and exit. False","title":"import-env"},{"location":"commands/ogc/#ogc-init","text":"Initialize OGC Usage: ogc init [OPTIONS] Options: Name Type Description Default --help boolean Show this message and exit. False","title":"init"},{"location":"commands/ogc/#ogc-inspect","text":"List nodes in your inventory Usage: ogc inspect [OPTIONS] Options: Name Type Description Default --id text Inspect node by DB ID None --name text Inspect nodes by name, this can be a substring match None --tag text Inspect nodes by tag None --action-id text If set will only show the action output for a specific action ID None --extend / --no-extend boolean Show extended action output at once False --help boolean Show this message and exit. False","title":"inspect"},{"location":"commands/ogc/#ogc-launch","text":"Launches nodes from a provision specification Usage: ogc launch [OPTIONS] Options: Name Type Description Default --spec text N/A None --with-deploy / --with-no-deploy boolean Also performs script deployments True --help boolean Show this message and exit. False","title":"launch"},{"location":"commands/ogc/#ogc-ls","text":"List nodes in your inventory Usage: ogc ls [OPTIONS] Options: Name Type Description Default --by-tag text List nodes by tag None --by-name text List nodes by name, this can be a substring match None --output-file text Stores the table output to svg or html. Determined by the file extension. None --help boolean Show this message and exit. False","title":"ls"},{"location":"commands/ogc/#ogc-ls-key-pairs","text":"List keypairs Usage: ogc ls-key-pairs [OPTIONS] Options: Name Type Description Default --filter text Filter by keypair name None --provider text Provider to query aws --help boolean Show this message and exit. False","title":"ls-key-pairs"},{"location":"commands/ogc/#ogc-pull-artifacts","text":"Download artifacts from node Usage: ogc pull-artifacts [OPTIONS] NAME Options: Name Type Description Default --help boolean Show this message and exit. False","title":"pull-artifacts"},{"location":"commands/ogc/#ogc-pull-files","text":"Scp files or directories from node Usage: ogc pull-files [OPTIONS] NAME DST SRC Options: Name Type Description Default --help boolean Show this message and exit. False","title":"pull-files"},{"location":"commands/ogc/#ogc-push-files","text":"Scp files or directories to node Usage: ogc push-files [OPTIONS] NAME SRC DST Options: Name Type Description Default --exclude text Exclude files/directories when uploading None --help boolean Show this message and exit. False","title":"push-files"},{"location":"commands/ogc/#ogc-rm","text":"Destroys a node and its associated keys, storage, etc. Usage: ogc rm [OPTIONS] Options: Name Type Description Default --by-name text Remove node by its Name None --force / --no-force boolean Force removal regardless of connectivity False --only-db / --no-only-db boolean Force removal of database records only False --help boolean Show this message and exit. False","title":"rm"},{"location":"commands/ogc/#ogc-rm-all","text":"Destroys everything. Use with caution. Usage: ogc rm-all [OPTIONS] Options: Name Type Description Default --force / --no-force boolean Force removal regardless of connectivity False --only-db / --no-only-db boolean Force removal of database records only False --help boolean Show this message and exit. False","title":"rm-all"},{"location":"commands/ogc/#ogc-rm-key-pairs","text":"Remove keypairs Usage: ogc rm-key-pairs [OPTIONS] Options: Name Type Description Default --filter text Filter by keypair name None --provider text Provider to query aws --help boolean Show this message and exit. False","title":"rm-key-pairs"},{"location":"commands/ogc/#ogc-shell","text":"Launches IPython REPL Usage: ogc shell [OPTIONS] Options: Name Type Description Default --help boolean Show this message and exit. False","title":"shell"},{"location":"commands/ogc/#ogc-ssh","text":"Login to a node Usage: ogc ssh [OPTIONS] Options: Name Type Description Default --by-id text Login to a node by its ID None --by-name text Login to a node by its Name None --help boolean Show this message and exit. False","title":"ssh"},{"location":"commands/ogc/#ogc-status","text":"Get status of deployment Usage: ogc status [OPTIONS] Options: Name Type Description Default --reconcile / --no-reconcile boolean Attempt to fix deployment to match scale False --spec text N/A None --output-file text Stores the table output to svg or html. Determined by the file extension. None --help boolean Show this message and exit. False","title":"status"},{"location":"developer-guide/managing-nodes/","text":"Managing Nodes Programatically \u00b6 Requirements \u00b6 Accessing the functionality of OGC programatically requires that both cloud credentials and database access are configured. The environment variables for working with AWS or Google should be defined in your environment either by setting it in the .env or in the abscence of a dotenv file they can be exported by your current running shell. Using the .env is easiest and is what we'll use for the remaining documentation, the following will configure access to both AWS and Google along with defining where our Postgres database resides: AWS_ACCESS_KEY_ID = \"abbcc\" AWS_SECRET_ACCESS_KEY = \"sshitsasecret\" AWS_REGION = \"us-east-2\" GOOGLE_APPLICATION_CREDENTIALS = \"mycreds.json\" GOOGLE_APPLICATION_SERVICE_ACCOUNT = \"bob@whodunit.iam.gserviceaccount.com\" GOOGLE_PROJECT = \"my-awesome-project\" GOOGLE_DATACENTER = \"us-central1-a\" POSTGRES_HOST = \"localhost\" POSTGRES_PORT = 5432 POSTGRES_DB = \"ogc\" POSTGRES_USER = \"postgres\" POSTGRES_PASSWORD = \"postgres\" Database \u00b6 Connect \u00b6 Everything is tracked in the database, first thing to do is setup that database connection. from ogc import db , state if not state . app . engine : state . app . engine = db . connect () state . app . session = db . session ( state . app . engine ) Initialize Tables \u00b6 Next, create the database tables, OGC provides a helper for this: db . createtbl ( state . app . engine ) Additionally, dropping the tables can be achieved this way: db . droptbl ( state . app . engine ) Caution If altering the database models make sure to perform migrations for users who upgrade OGC versions. First create the migration: $ alembic revision --autogenerate -m \"A new modification made\" Next perform the migration $ alembic upgrade head Or programatically: from ogc import db db . migrate () See Alembic auto generating migrations for more info. Create User \u00b6 A single user record is required in the database, this allows OGC to track cloud resources by certain tags associated with the OGC user. To create an initial user: from slugify import slugify with session as s : has_user = s . select ( db . User ) . first () or None if not has_user : user = db . User ( name = \"adam\" , slug = slugify ( name )) s . add ( user ) s . commit () return print ( \"User exists, nothing to do here\" ) Querying the database uses standard SQLAlchemy , please reference that site for additional information. Nodes \u00b6 Once the database is setup in your code, you are ready to begin creating and managing nodes. OGC provides both synchronous and asynchronous support depending on your needs. Launch Node \u00b6 To launch a node an OGC specification is required with at least one layout defined. from ogc.spec import SpecLoader app . spec = SpecLoader . load ([ \"/Users/adam/specs/ogc.yml\" ]) To launch this node layout synchronously: from ogc import actions node_ids_created = [ actions . launch ( layout . as_dict ()) for layout in app . spec . layouts ] For an asynchronous version: from ogc import actions node_ids_created = actions . launch_async ( app . spec . layouts ) Info The naming conventions used for async functions is to append the suffix of async to the synchronous function name, for example, actions.exec and actions.exec_async . Script Deployment \u00b6 Launching and deploying are separated into two parts, this allows for further customization between bringing up a machine and letting OGC handle the remaining deployment options. To deploy the scripts defined in your specification, use the results from the previous launch of node_ids_created : script_deploy_results = actions . deploy_async ( node_ids_created ) Checking Results \u00b6 Checking the results of the deployment can be done in this way: if all ( result == True for result in script_deploy_results ): print ( \"Successfully deployed\" ) else : print ( \"One or more deployments failed\" )","title":"Managing nodes"},{"location":"developer-guide/managing-nodes/#managing-nodes-programatically","text":"","title":"Managing Nodes Programatically"},{"location":"developer-guide/managing-nodes/#requirements","text":"Accessing the functionality of OGC programatically requires that both cloud credentials and database access are configured. The environment variables for working with AWS or Google should be defined in your environment either by setting it in the .env or in the abscence of a dotenv file they can be exported by your current running shell. Using the .env is easiest and is what we'll use for the remaining documentation, the following will configure access to both AWS and Google along with defining where our Postgres database resides: AWS_ACCESS_KEY_ID = \"abbcc\" AWS_SECRET_ACCESS_KEY = \"sshitsasecret\" AWS_REGION = \"us-east-2\" GOOGLE_APPLICATION_CREDENTIALS = \"mycreds.json\" GOOGLE_APPLICATION_SERVICE_ACCOUNT = \"bob@whodunit.iam.gserviceaccount.com\" GOOGLE_PROJECT = \"my-awesome-project\" GOOGLE_DATACENTER = \"us-central1-a\" POSTGRES_HOST = \"localhost\" POSTGRES_PORT = 5432 POSTGRES_DB = \"ogc\" POSTGRES_USER = \"postgres\" POSTGRES_PASSWORD = \"postgres\"","title":"Requirements"},{"location":"developer-guide/managing-nodes/#database","text":"","title":"Database"},{"location":"developer-guide/managing-nodes/#connect","text":"Everything is tracked in the database, first thing to do is setup that database connection. from ogc import db , state if not state . app . engine : state . app . engine = db . connect () state . app . session = db . session ( state . app . engine )","title":"Connect"},{"location":"developer-guide/managing-nodes/#initialize-tables","text":"Next, create the database tables, OGC provides a helper for this: db . createtbl ( state . app . engine ) Additionally, dropping the tables can be achieved this way: db . droptbl ( state . app . engine ) Caution If altering the database models make sure to perform migrations for users who upgrade OGC versions. First create the migration: $ alembic revision --autogenerate -m \"A new modification made\" Next perform the migration $ alembic upgrade head Or programatically: from ogc import db db . migrate () See Alembic auto generating migrations for more info.","title":"Initialize Tables"},{"location":"developer-guide/managing-nodes/#create-user","text":"A single user record is required in the database, this allows OGC to track cloud resources by certain tags associated with the OGC user. To create an initial user: from slugify import slugify with session as s : has_user = s . select ( db . User ) . first () or None if not has_user : user = db . User ( name = \"adam\" , slug = slugify ( name )) s . add ( user ) s . commit () return print ( \"User exists, nothing to do here\" ) Querying the database uses standard SQLAlchemy , please reference that site for additional information.","title":"Create User"},{"location":"developer-guide/managing-nodes/#nodes","text":"Once the database is setup in your code, you are ready to begin creating and managing nodes. OGC provides both synchronous and asynchronous support depending on your needs.","title":"Nodes"},{"location":"developer-guide/managing-nodes/#launch-node","text":"To launch a node an OGC specification is required with at least one layout defined. from ogc.spec import SpecLoader app . spec = SpecLoader . load ([ \"/Users/adam/specs/ogc.yml\" ]) To launch this node layout synchronously: from ogc import actions node_ids_created = [ actions . launch ( layout . as_dict ()) for layout in app . spec . layouts ] For an asynchronous version: from ogc import actions node_ids_created = actions . launch_async ( app . spec . layouts ) Info The naming conventions used for async functions is to append the suffix of async to the synchronous function name, for example, actions.exec and actions.exec_async .","title":"Launch Node"},{"location":"developer-guide/managing-nodes/#script-deployment","text":"Launching and deploying are separated into two parts, this allows for further customization between bringing up a machine and letting OGC handle the remaining deployment options. To deploy the scripts defined in your specification, use the results from the previous launch of node_ids_created : script_deploy_results = actions . deploy_async ( node_ids_created )","title":"Script Deployment"},{"location":"developer-guide/managing-nodes/#checking-results","text":"Checking the results of the deployment can be done in this way: if all ( result == True for result in script_deploy_results ): print ( \"Successfully deployed\" ) else : print ( \"One or more deployments failed\" )","title":"Checking Results"},{"location":"developer-guide/api/actions/","text":"API \u00b6 ogc.actions \u00b6 deploy ( node_id ) \u00b6 Execute the deployment Function for executing the deployment on a node. Synopsis: 1 2 3 4 5 6 7 from ogc.spec import SpecLoader from ogc import actions app.spec = SpecLoader.load([\"/Users/adam/specs/ogc.yml\"]) layout = app.spec.layouts[0] node_ids = actions.launch(layout.as_dict()) script_deploy_results = actions.deploy(node_id) Parameters: Name Type Description Default node int The node ID from the launch required Returns: Type Description bool True if successful, False otherwise. deploy_async ( nodes ) \u00b6 Execute the deployment Asynchronous function for executing the deployment on a node. Synopsis: 1 2 3 4 5 6 from ogc.spec import SpecLoader from ogc import actions app.spec = SpecLoader.load([\"/Users/adam/specs/ogc.yml\"]) node_ids = actions.launch_async(app.spec.layouts) script_deploy_results = actions.deploy_async(node_ids) Parameters: Name Type Description Default nodes list[int] The node id's from the launch required Returns: Type Description list[bool] A list of booleans from result of deployment. exec ( node , cmd ) \u00b6 Execute command on Node Function for executing a command on a node. Synopsis: 1 2 3 4 5 from ogc import actions, state, db node = db.query(db.Node).first() actions.exec(node, \"ls -l /\") for action in node.actions: print(action.exit_code, action.out, action.error) Parameters: Name Type Description Default node ogc.db.Node The node to execute a command on required cmd str The command string required Returns: Type Description bool True if succesful, False otherwise exec_async ( name , tag , cmd ) \u00b6 Execute command on Nodes Async function for executing a command on a node. Synopsis: 1 2 3 4 from ogc import actions, state, db node = db.query(db.Node).filter(db.Node.tags.contains([tag])) results = actions.exec_async(node, \"ls -l /\") all(result == True for result in results) Parameters: Name Type Description Default name str The node name to execute a command on required tag str The tag to query for nodes. Allows running commands across multiple nodes. required cmd str The command string required Returns: Type Description list[bool] True if all execs complete succesfully, False otherwise. exec_scripts ( node , path ) \u00b6 Execute a scripts/template directory on a Node Function for executing scripts/templates on a node. Synopsis: 1 2 3 4 from ogc import actions, state, db node = db.query(db.Node).first() result = actions.exec_scripts(node, \"templates/deploy/ubuntu\") result == True Parameters: Name Type Description Default node ogc.db.Node The node to execute scripts on required path str The path where the scripts reside locally required Returns: Type Description bool True if succesful, False otherwise. exec_scripts_async ( name , tag , path ) \u00b6 Execute a scripts/template directory on a Node Async function for executing scripts/templates on a node. Synopsis: 1 2 3 4 from ogc import actions, state, db nodes = db.query(db.Node).all() results = actions.exec_scripts_async(nodes, \"templates/deploy/ubuntu\") all(result == True for result in results) Parameters: Name Type Description Default name str The node name to execute scripts on required tag str The node tag to query, allows running across multiple nodes. required path str The path where the scripts reside locally required Returns: Type Description list[bool] True if succesful, False otherwise. launch ( layout ) \u00b6 Launch a node. Synchronous function for launching a node in a cloud environment. Synopsis: 1 2 3 4 5 from ogc.spec import SpecLoader from ogc import actions app.spec = SpecLoader.load([\"/Users/adam/specs/ogc.yml\"]) node_ids_created = [actions.launch(layout.as_dict()) for layout in app.spec.layouts] Parameters: Name Type Description Default layout ogc.spec.SpecProvisionLayout The layout specification used when launching a node. required Returns: Type Description id (int) The database row ID of the node that was deployed. launch_async ( layouts ) \u00b6 Launch a node asynchronously. Asynchronous function for launching a node in a cloud environment. Synopsis: 1 2 3 4 5 from ogc.spec import SpecLoader from ogc import actions app.spec = SpecLoader.load([\"/Users/adam/specs/ogc.yml\"]) node_ids_created = actions.launch_async(app.spec.layouts) Parameters: Name Type Description Default layouts list[ogc.spec.SpecProvisionLayout] The layout specification used when launching a node. required Returns: Type Description ids (list[int]) The database row ID's of the node(s) that were deployed. sync ( layout , overrides ) \u00b6 Sync a deployment Function for syncing a deployment to correct scale. Synopsis: 1 2 3 4 from ogc import actions, state layout = app.spec.layouts[0] result = actions.sync(layout, overrides={'elastic-agent-ubuntu': {'action': 'add', remaining: 5}}) result == True Parameters: Name Type Description Default layout ogc.spec.SpecProvisionLayout The layout of the deployment required overrides dict Override dictionary of what the new count of nodes should be required Returns: Type Description bool True if synced, False otherwise sync_async ( layouts , overrides ) \u00b6 Sync a deployment Async function for syncing a deployment to correct scale. Synopsis: 1 2 3 4 from ogc import actions, state results = actions.sync_async(app.spec.layouts, overrides={'elastic-agent-ubuntu': {'action': 'add', remaining: 5}}) all(result == True for result in results) Parameters: Name Type Description Default layout ogc.spec.SpecProvisionLayout The layout of the deployment required overrides dict Override dictionary of what the new count of nodes should be required Returns: Type Description list[bool] True if synced, False otherwise teardown ( name , force = False , only_db = False ) \u00b6 Teardown deployment Function for tearing down a node. Synopsis: 1 2 3 from ogc import actions name = \"ogc-234342-elastic-agent-ubuntu\" is_down = actions.teardown(name, force=True) Parameters: Name Type Description Default name str The node name to teardown required force bool Force False only_db bool Will remove from database regardless of cloud state. Use with caution. False Returns: Type Description bool True if teardown is successful, False otherwise. teardown_async ( names , force = False , only_db = False ) \u00b6 Teardown deployment Async function for tearing down a node. Synopsis: 1 2 3 from ogc import actions names = [\"ogc-234342-elastic-agent-ubuntu\", \"ogc-abce34-kibana-ubuntu\"] is_down = actions.teardown_async(names, force=True) Parameters: Name Type Description Default name list[str] The node name to teardown required force bool Force False only_db bool Will remove from database regardless of cloud state. Use with caution. False Returns: Type Description list[bool] True if teardown is successful, False otherwise.","title":"ogc"},{"location":"developer-guide/api/actions/#api","text":"","title":"API"},{"location":"developer-guide/api/actions/#ogc.actions","text":"","title":"actions"},{"location":"developer-guide/api/actions/#ogc.actions.deploy","text":"Execute the deployment Function for executing the deployment on a node. Synopsis: 1 2 3 4 5 6 7 from ogc.spec import SpecLoader from ogc import actions app.spec = SpecLoader.load([\"/Users/adam/specs/ogc.yml\"]) layout = app.spec.layouts[0] node_ids = actions.launch(layout.as_dict()) script_deploy_results = actions.deploy(node_id) Parameters: Name Type Description Default node int The node ID from the launch required Returns: Type Description bool True if successful, False otherwise.","title":"deploy()"},{"location":"developer-guide/api/actions/#ogc.actions.deploy_async","text":"Execute the deployment Asynchronous function for executing the deployment on a node. Synopsis: 1 2 3 4 5 6 from ogc.spec import SpecLoader from ogc import actions app.spec = SpecLoader.load([\"/Users/adam/specs/ogc.yml\"]) node_ids = actions.launch_async(app.spec.layouts) script_deploy_results = actions.deploy_async(node_ids) Parameters: Name Type Description Default nodes list[int] The node id's from the launch required Returns: Type Description list[bool] A list of booleans from result of deployment.","title":"deploy_async()"},{"location":"developer-guide/api/actions/#ogc.actions.exec","text":"Execute command on Node Function for executing a command on a node. Synopsis: 1 2 3 4 5 from ogc import actions, state, db node = db.query(db.Node).first() actions.exec(node, \"ls -l /\") for action in node.actions: print(action.exit_code, action.out, action.error) Parameters: Name Type Description Default node ogc.db.Node The node to execute a command on required cmd str The command string required Returns: Type Description bool True if succesful, False otherwise","title":"exec()"},{"location":"developer-guide/api/actions/#ogc.actions.exec_async","text":"Execute command on Nodes Async function for executing a command on a node. Synopsis: 1 2 3 4 from ogc import actions, state, db node = db.query(db.Node).filter(db.Node.tags.contains([tag])) results = actions.exec_async(node, \"ls -l /\") all(result == True for result in results) Parameters: Name Type Description Default name str The node name to execute a command on required tag str The tag to query for nodes. Allows running commands across multiple nodes. required cmd str The command string required Returns: Type Description list[bool] True if all execs complete succesfully, False otherwise.","title":"exec_async()"},{"location":"developer-guide/api/actions/#ogc.actions.exec_scripts","text":"Execute a scripts/template directory on a Node Function for executing scripts/templates on a node. Synopsis: 1 2 3 4 from ogc import actions, state, db node = db.query(db.Node).first() result = actions.exec_scripts(node, \"templates/deploy/ubuntu\") result == True Parameters: Name Type Description Default node ogc.db.Node The node to execute scripts on required path str The path where the scripts reside locally required Returns: Type Description bool True if succesful, False otherwise.","title":"exec_scripts()"},{"location":"developer-guide/api/actions/#ogc.actions.exec_scripts_async","text":"Execute a scripts/template directory on a Node Async function for executing scripts/templates on a node. Synopsis: 1 2 3 4 from ogc import actions, state, db nodes = db.query(db.Node).all() results = actions.exec_scripts_async(nodes, \"templates/deploy/ubuntu\") all(result == True for result in results) Parameters: Name Type Description Default name str The node name to execute scripts on required tag str The node tag to query, allows running across multiple nodes. required path str The path where the scripts reside locally required Returns: Type Description list[bool] True if succesful, False otherwise.","title":"exec_scripts_async()"},{"location":"developer-guide/api/actions/#ogc.actions.launch","text":"Launch a node. Synchronous function for launching a node in a cloud environment. Synopsis: 1 2 3 4 5 from ogc.spec import SpecLoader from ogc import actions app.spec = SpecLoader.load([\"/Users/adam/specs/ogc.yml\"]) node_ids_created = [actions.launch(layout.as_dict()) for layout in app.spec.layouts] Parameters: Name Type Description Default layout ogc.spec.SpecProvisionLayout The layout specification used when launching a node. required Returns: Type Description id (int) The database row ID of the node that was deployed.","title":"launch()"},{"location":"developer-guide/api/actions/#ogc.actions.launch_async","text":"Launch a node asynchronously. Asynchronous function for launching a node in a cloud environment. Synopsis: 1 2 3 4 5 from ogc.spec import SpecLoader from ogc import actions app.spec = SpecLoader.load([\"/Users/adam/specs/ogc.yml\"]) node_ids_created = actions.launch_async(app.spec.layouts) Parameters: Name Type Description Default layouts list[ogc.spec.SpecProvisionLayout] The layout specification used when launching a node. required Returns: Type Description ids (list[int]) The database row ID's of the node(s) that were deployed.","title":"launch_async()"},{"location":"developer-guide/api/actions/#ogc.actions.sync","text":"Sync a deployment Function for syncing a deployment to correct scale. Synopsis: 1 2 3 4 from ogc import actions, state layout = app.spec.layouts[0] result = actions.sync(layout, overrides={'elastic-agent-ubuntu': {'action': 'add', remaining: 5}}) result == True Parameters: Name Type Description Default layout ogc.spec.SpecProvisionLayout The layout of the deployment required overrides dict Override dictionary of what the new count of nodes should be required Returns: Type Description bool True if synced, False otherwise","title":"sync()"},{"location":"developer-guide/api/actions/#ogc.actions.sync_async","text":"Sync a deployment Async function for syncing a deployment to correct scale. Synopsis: 1 2 3 4 from ogc import actions, state results = actions.sync_async(app.spec.layouts, overrides={'elastic-agent-ubuntu': {'action': 'add', remaining: 5}}) all(result == True for result in results) Parameters: Name Type Description Default layout ogc.spec.SpecProvisionLayout The layout of the deployment required overrides dict Override dictionary of what the new count of nodes should be required Returns: Type Description list[bool] True if synced, False otherwise","title":"sync_async()"},{"location":"developer-guide/api/actions/#ogc.actions.teardown","text":"Teardown deployment Function for tearing down a node. Synopsis: 1 2 3 from ogc import actions name = \"ogc-234342-elastic-agent-ubuntu\" is_down = actions.teardown(name, force=True) Parameters: Name Type Description Default name str The node name to teardown required force bool Force False only_db bool Will remove from database regardless of cloud state. Use with caution. False Returns: Type Description bool True if teardown is successful, False otherwise.","title":"teardown()"},{"location":"developer-guide/api/actions/#ogc.actions.teardown_async","text":"Teardown deployment Async function for tearing down a node. Synopsis: 1 2 3 from ogc import actions names = [\"ogc-234342-elastic-agent-ubuntu\", \"ogc-abce34-kibana-ubuntu\"] is_down = actions.teardown_async(names, force=True) Parameters: Name Type Description Default name list[str] The node name to teardown required force bool Force False only_db bool Will remove from database regardless of cloud state. Use with caution. False Returns: Type Description list[bool] True if teardown is successful, False otherwise.","title":"teardown_async()"},{"location":"user-guide/defining-layouts/","text":"Defining Layouts \u00b6 Learn the layout specification and how to create your own provisioning layouts. All layouts reside under the layouts key in the provision specification: layouts : elastic-agent-sles : runs-on : sles-15 instance-size : e2-standard-8 username : ogc scripts : fixtures/ex_deploy_sles provider : google scale : 5 remote-path : /home/ogc/ogc include : - .ogc-cache exclude : - .git - .venv artifacts : /home/ogc/output/*.xml tags : - elastic-agent-8.1.x - sles-gcp ports : - \"80:80\" - \"443:443\" Each layout has a friendly name associated as seen by elastic-agent-sles . The next section is going to go over each option and describe its meaning. provider Define which cloud the layout will operate in. Currently supported options are aws and google . runs-on Define the base OS image to be deployed on to the nodes. The current supported list of names are: AWS Google ubuntu-2004 ubuntu-2004 ubuntu-1804 ubuntu-1804 ubuntu-1604 ubuntu-1604 sles-15 sles-15 sles-12 sles-12 sles-11 sles-11 debian-10 debian-10 debian-9 debian-9 debian-8 debian-8 rhel-8 rhel-7 rhel-6 instance-size Define the machine size, this is dependent on which provider is chosen. The instance-size correlates with the instance size naming for each cloud. For example, on AWS you would use instance-size: c5.4xlarge and in Google's case, instance-size: e2-standard-8 . username The ssh user to use when deploying and accessing the nodes. This is also somewhat dependent on which provider is used. In the case of Google , any username can be given. In the case of AWS , the base machines have a pre-loaded user that must be used: AWS Username centos centos debian admin oracle ec2-user sles ec2-user ubuntu ubuntu Caution A lot of cloud machine images disable root login, try to avoid using that as a user and utilize sudo for anything requiring elevated permissions. scripts The location on your machine where templates/scripts resides. These will be uploaded and executed during the deployment phase. Note See scripting for more information. scale How many nodes of each layout to deploy. This is also referenced during a deployment reconciliation phase. remote-path (optional) If set, any uploads/downloads outside of what's defined in scripts will be placed in that remote path. include (optional) A list of files/wildcards to include in the upload exclude (optional) A list of files/wildcards to exclude in the upload artifacts (optional) The remote path where script execution output is stored. This is used when pulling artifacts with ogc pull-artifacts and also utilized during node teardown. This will download any artifacts found into artifacts/instance-name/ . tags (optional) Define tags for each layout, allows additional filtering capabilities and deployment options when used with ogc ls and ogc exec ports (optional) Define what ingress ports are available when accessing the node. Variants \u00b6 OGC supports the concept of variants. In OGC's case, variants are handled by multiple provision spec files and are then merged in a merge-left fashion. What this means is that we can take a base spec file such as: name : ci ssh-keys : public : id_rsa.pub private : id_rsa layouts : elastic-agent-sles : runs-on : sles-15 instance-size : e2-standard-8 username : ogc scripts : fixtures/ex_deploy_sles provider : google scale : 5 remote-path : /home/ogc/ogc include : - .ogc-cache exclude : - .git - .venv artifacts : /home/ogc/output/*.xml tags : - elastic-agent-8.1.x - sles-gcp elastic-agent-ubuntu : runs-on : ubuntu-latest instance-size : e2-standard-8 username : ogc scripts : fixtures/ex_deploy_ubuntu provider : google scale : 5 remote-path : /home/ogc/ogc exclude : - .git - .venv artifacts : /home/ogc/output/*.xml tags : - elastic-agent-8.1.x - ubuntu-gcp The name of the file doesn't matter, we'll call this file base-spec.yml . Now if we need to change certain aspects of this base deploy specification we can define a second YAML file, we'll call it ubuntu-1804-no-sles.yml . In this example, let's change the username and runs-on for the ubuntu layout, and let's also remove the sles layout: layouts : elastic-agent-sles : {} elastic-agent-ubuntu : runs-on : ubuntu-1804 username : ubuntu The merging of the specifications will remove any keys that exist if the value of the key is {} (signaling an empty stanza). The remaining keys that match up with the original spec will then be overridden and the rest of the specification is left untouched. Adding new layouts is just a matter of defining another section under layouts in the additional spec files passed to OGC.","title":"Defining Layouts"},{"location":"user-guide/defining-layouts/#defining-layouts","text":"Learn the layout specification and how to create your own provisioning layouts. All layouts reside under the layouts key in the provision specification: layouts : elastic-agent-sles : runs-on : sles-15 instance-size : e2-standard-8 username : ogc scripts : fixtures/ex_deploy_sles provider : google scale : 5 remote-path : /home/ogc/ogc include : - .ogc-cache exclude : - .git - .venv artifacts : /home/ogc/output/*.xml tags : - elastic-agent-8.1.x - sles-gcp ports : - \"80:80\" - \"443:443\" Each layout has a friendly name associated as seen by elastic-agent-sles . The next section is going to go over each option and describe its meaning. provider Define which cloud the layout will operate in. Currently supported options are aws and google . runs-on Define the base OS image to be deployed on to the nodes. The current supported list of names are: AWS Google ubuntu-2004 ubuntu-2004 ubuntu-1804 ubuntu-1804 ubuntu-1604 ubuntu-1604 sles-15 sles-15 sles-12 sles-12 sles-11 sles-11 debian-10 debian-10 debian-9 debian-9 debian-8 debian-8 rhel-8 rhel-7 rhel-6 instance-size Define the machine size, this is dependent on which provider is chosen. The instance-size correlates with the instance size naming for each cloud. For example, on AWS you would use instance-size: c5.4xlarge and in Google's case, instance-size: e2-standard-8 . username The ssh user to use when deploying and accessing the nodes. This is also somewhat dependent on which provider is used. In the case of Google , any username can be given. In the case of AWS , the base machines have a pre-loaded user that must be used: AWS Username centos centos debian admin oracle ec2-user sles ec2-user ubuntu ubuntu Caution A lot of cloud machine images disable root login, try to avoid using that as a user and utilize sudo for anything requiring elevated permissions. scripts The location on your machine where templates/scripts resides. These will be uploaded and executed during the deployment phase. Note See scripting for more information. scale How many nodes of each layout to deploy. This is also referenced during a deployment reconciliation phase. remote-path (optional) If set, any uploads/downloads outside of what's defined in scripts will be placed in that remote path. include (optional) A list of files/wildcards to include in the upload exclude (optional) A list of files/wildcards to exclude in the upload artifacts (optional) The remote path where script execution output is stored. This is used when pulling artifacts with ogc pull-artifacts and also utilized during node teardown. This will download any artifacts found into artifacts/instance-name/ . tags (optional) Define tags for each layout, allows additional filtering capabilities and deployment options when used with ogc ls and ogc exec ports (optional) Define what ingress ports are available when accessing the node.","title":"Defining Layouts"},{"location":"user-guide/defining-layouts/#variants","text":"OGC supports the concept of variants. In OGC's case, variants are handled by multiple provision spec files and are then merged in a merge-left fashion. What this means is that we can take a base spec file such as: name : ci ssh-keys : public : id_rsa.pub private : id_rsa layouts : elastic-agent-sles : runs-on : sles-15 instance-size : e2-standard-8 username : ogc scripts : fixtures/ex_deploy_sles provider : google scale : 5 remote-path : /home/ogc/ogc include : - .ogc-cache exclude : - .git - .venv artifacts : /home/ogc/output/*.xml tags : - elastic-agent-8.1.x - sles-gcp elastic-agent-ubuntu : runs-on : ubuntu-latest instance-size : e2-standard-8 username : ogc scripts : fixtures/ex_deploy_ubuntu provider : google scale : 5 remote-path : /home/ogc/ogc exclude : - .git - .venv artifacts : /home/ogc/output/*.xml tags : - elastic-agent-8.1.x - ubuntu-gcp The name of the file doesn't matter, we'll call this file base-spec.yml . Now if we need to change certain aspects of this base deploy specification we can define a second YAML file, we'll call it ubuntu-1804-no-sles.yml . In this example, let's change the username and runs-on for the ubuntu layout, and let's also remove the sles layout: layouts : elastic-agent-sles : {} elastic-agent-ubuntu : runs-on : ubuntu-1804 username : ubuntu The merging of the specifications will remove any keys that exist if the value of the key is {} (signaling an empty stanza). The remaining keys that match up with the original spec will then be overridden and the rest of the specification is left untouched. Adding new layouts is just a matter of defining another section under layouts in the additional spec files passed to OGC.","title":"Variants"},{"location":"user-guide/managing-nodes/","text":"Managing a Deployment \u00b6 Learn how to list, inspect, access and debug your node deployments. Listing Nodes \u00b6 To list nodes in your deployment, run the following: $ ogc ls Which gives a table output of current node deployments: You can further drill down with a couple of options: To filter by-tag run: $ ogc ls --by-tag user-adam Accessing nodes \u00b6 OGC provides a helper command for easily accessing any of the nodes in your deployment. To login to one of the above nodes ogc-d7cd61a7-elastic-agent-ubuntu run: $ ogc ssh --by-name ogc-d7cd61a7-elastic-agent-ubuntu ... ssh output ... ogc@ogc-d7cd61a7-elastic-agent-ubuntu:~# Alternatively, use the ID : $ ogc ssh --by-id 38 Executing commands \u00b6 Running arbitrary commands can be accomplished with: $ ogc exec --by-name ogc-d7cd61a7-elastic-agent-ubuntu 'ls -l /' Or if tags are defined, run a command across a set of machines: $ ogc exec --by-tag ubuntu-gcp 'touch this_is_an_ubuntu_machine.txt' Executing a scripts directory \u00b6 In addition to running arbitrary commands, OGC can also execute a directory of templates/scripts: $ ogc exec-scripts --by-name ogc-d7cd61a7-elastic-agent-ubuntu fixtures/ex_deploy_ubuntu Or if tags are defined, run across a set of machines: $ ogc exec-scripts --by-tag ubuntu-gcp fixtures/ex_deploy_ubuntu This can be useful to re-run a deployment or add new functionality/one-offs to a node without disturbing the original layout specifications. Access to the database and all templating is available as well. Downloading files \u00b6 There are 2 ways to download files, the first is to use ogc pull-files , this gives you the ability to download any arbitrary files: $ ogc pull-files ogc-d7cd61a7-elastic-agent-ubuntu im_on_a_computer.txt im_downloaded_computer.txt $ stat im_downloaded_computer.txt 16777221 24809112 -rw-r--r-- 1 adam staff 0 0 \"Mar 24 11:56:24 2022\" \"Mar 24 11:55:16 2022\" \"Mar 24 11:56:24 2022\" \"Mar 24 11:55:16 2022\" 4096 0 0 im_downloaded_computer.txt Another way is if the artifacts key is defined in a layout. To grab files defined by that artifacts option run the following: $ ogc pull-artifacts ogc-d7cd61a7-elastic-agent-ubuntu By default, artifacts are stored in $(pwd)/artifacts/ogc-d7cd61a7-elastic-agent-ubuntu tree artifacts/ogc-d7cd61a7-elastic-agent-ubuntu/ artifacts/ogc-d7cd61a7-elastic-agent-ubuntu/ \u2514\u2500\u2500 test.xml 0 directories, 1 file Uploading files \u00b6 OGC provides a simple way to upload arbitrary files to a node: $ ogc push-files ogc-d7cd61a7-elastic-agent-ubuntu im_downloaded_computer.txt dl.txt Optionally, if --exclude is provided, uploading files will ignore any wildcards matched. Note Passing multiple --exclude is supported and will be added to the list of excludes during upload. Useful if uploading directories and want to ignore things like .git and .venv . Inspecting nodes \u00b6 Each action performed on a node is tracked. This allows you to quickly investigate why scripts failed. To inspect a node and see action results run: $ ogc inspect --id 38 This will return the following output: Deploy Details: ogc-d7cd61a7-elastic-agent-ubuntu [3] Successful Actions: (id: 90) Out: 2022-03-24 12:37:08.657289 '/usr/local/bin/pacman' -> '/usr/local/bin/pacapt' Reading package lists... Building dependency tree... Reading state information... nano is already the newest version (2.9.3-2). nano set to manually installed. The following package was automatically installed and is no longer required: libnuma1 Use 'apt autoremove' to remove it. 0 upgraded, 0 newly installed, 0 to remove and 0 not upgraded. If multiple actions exist, further drill down into the action you want ( seen here \"(id: 90) Out: 2022-03-24 12:37:08.\" in our example ): $ ogc inspect --id 38 --action-id 90 Syncing a deployment \u00b6 In some cases nodes will fail to deploy or you remembered you needed more than 5 nodes or maybe you need less nodes than what the original scale was set. In all these cases, OGC provides a way to keep the deployment in sync with the layouts. To get an idea of the health of the deployment, run: $ ogc status The output returned will be a table displaying what's deployed, the scale, and if there are any remaining nodes left: In cases where you want to add more nodes, update your layout and increase the scale option, in this case we want to add 10 more nodes to our elastic-agent-sles layout: Or another case where we need to reduce the number of nodes from 5 to 3: To perform the sync, run the following: $ ogc status --reconcile And the output will show OGC destroying 2 nodes from each layout: 2022-03-24 at 11:52:37 | INFO Reconciling: [elastic-agent-sles, elastic-agent-ubuntu] 2022-03-24 at 11:52:37 | INFO Destroying: ogc-87ba30fc-elastic-agent-sles 2022-03-24 at 11:52:37 | INFO Destroying: ogc-51b971ad-elastic-agent-sles 2022-03-24 at 11:52:37 | INFO Destroying: ogc-b3befadc-elastic-agent-ubuntu 2022-03-24 at 11:52:37 | INFO Destroying: ogc-d54a5848-elastic-agent-ubuntu Destroying nodes \u00b6 OGC allows destroying of individual or a full blown cleanup. To remove a single node we run: $ ogc rm --by-name ogc-d7cd61a7-elastic-agent-ubuntu --force Or if we wanted to do a full teardown, run: $ ogc rm-all --force","title":"Managing a deployment"},{"location":"user-guide/managing-nodes/#managing-a-deployment","text":"Learn how to list, inspect, access and debug your node deployments.","title":"Managing a Deployment"},{"location":"user-guide/managing-nodes/#listing-nodes","text":"To list nodes in your deployment, run the following: $ ogc ls Which gives a table output of current node deployments: You can further drill down with a couple of options: To filter by-tag run: $ ogc ls --by-tag user-adam","title":"Listing Nodes"},{"location":"user-guide/managing-nodes/#accessing-nodes","text":"OGC provides a helper command for easily accessing any of the nodes in your deployment. To login to one of the above nodes ogc-d7cd61a7-elastic-agent-ubuntu run: $ ogc ssh --by-name ogc-d7cd61a7-elastic-agent-ubuntu ... ssh output ... ogc@ogc-d7cd61a7-elastic-agent-ubuntu:~# Alternatively, use the ID : $ ogc ssh --by-id 38","title":"Accessing nodes"},{"location":"user-guide/managing-nodes/#executing-commands","text":"Running arbitrary commands can be accomplished with: $ ogc exec --by-name ogc-d7cd61a7-elastic-agent-ubuntu 'ls -l /' Or if tags are defined, run a command across a set of machines: $ ogc exec --by-tag ubuntu-gcp 'touch this_is_an_ubuntu_machine.txt'","title":"Executing commands"},{"location":"user-guide/managing-nodes/#executing-a-scripts-directory","text":"In addition to running arbitrary commands, OGC can also execute a directory of templates/scripts: $ ogc exec-scripts --by-name ogc-d7cd61a7-elastic-agent-ubuntu fixtures/ex_deploy_ubuntu Or if tags are defined, run across a set of machines: $ ogc exec-scripts --by-tag ubuntu-gcp fixtures/ex_deploy_ubuntu This can be useful to re-run a deployment or add new functionality/one-offs to a node without disturbing the original layout specifications. Access to the database and all templating is available as well.","title":"Executing a scripts directory"},{"location":"user-guide/managing-nodes/#downloading-files","text":"There are 2 ways to download files, the first is to use ogc pull-files , this gives you the ability to download any arbitrary files: $ ogc pull-files ogc-d7cd61a7-elastic-agent-ubuntu im_on_a_computer.txt im_downloaded_computer.txt $ stat im_downloaded_computer.txt 16777221 24809112 -rw-r--r-- 1 adam staff 0 0 \"Mar 24 11:56:24 2022\" \"Mar 24 11:55:16 2022\" \"Mar 24 11:56:24 2022\" \"Mar 24 11:55:16 2022\" 4096 0 0 im_downloaded_computer.txt Another way is if the artifacts key is defined in a layout. To grab files defined by that artifacts option run the following: $ ogc pull-artifacts ogc-d7cd61a7-elastic-agent-ubuntu By default, artifacts are stored in $(pwd)/artifacts/ogc-d7cd61a7-elastic-agent-ubuntu tree artifacts/ogc-d7cd61a7-elastic-agent-ubuntu/ artifacts/ogc-d7cd61a7-elastic-agent-ubuntu/ \u2514\u2500\u2500 test.xml 0 directories, 1 file","title":"Downloading files"},{"location":"user-guide/managing-nodes/#uploading-files","text":"OGC provides a simple way to upload arbitrary files to a node: $ ogc push-files ogc-d7cd61a7-elastic-agent-ubuntu im_downloaded_computer.txt dl.txt Optionally, if --exclude is provided, uploading files will ignore any wildcards matched. Note Passing multiple --exclude is supported and will be added to the list of excludes during upload. Useful if uploading directories and want to ignore things like .git and .venv .","title":"Uploading files"},{"location":"user-guide/managing-nodes/#inspecting-nodes","text":"Each action performed on a node is tracked. This allows you to quickly investigate why scripts failed. To inspect a node and see action results run: $ ogc inspect --id 38 This will return the following output: Deploy Details: ogc-d7cd61a7-elastic-agent-ubuntu [3] Successful Actions: (id: 90) Out: 2022-03-24 12:37:08.657289 '/usr/local/bin/pacman' -> '/usr/local/bin/pacapt' Reading package lists... Building dependency tree... Reading state information... nano is already the newest version (2.9.3-2). nano set to manually installed. The following package was automatically installed and is no longer required: libnuma1 Use 'apt autoremove' to remove it. 0 upgraded, 0 newly installed, 0 to remove and 0 not upgraded. If multiple actions exist, further drill down into the action you want ( seen here \"(id: 90) Out: 2022-03-24 12:37:08.\" in our example ): $ ogc inspect --id 38 --action-id 90","title":"Inspecting nodes"},{"location":"user-guide/managing-nodes/#syncing-a-deployment","text":"In some cases nodes will fail to deploy or you remembered you needed more than 5 nodes or maybe you need less nodes than what the original scale was set. In all these cases, OGC provides a way to keep the deployment in sync with the layouts. To get an idea of the health of the deployment, run: $ ogc status The output returned will be a table displaying what's deployed, the scale, and if there are any remaining nodes left: In cases where you want to add more nodes, update your layout and increase the scale option, in this case we want to add 10 more nodes to our elastic-agent-sles layout: Or another case where we need to reduce the number of nodes from 5 to 3: To perform the sync, run the following: $ ogc status --reconcile And the output will show OGC destroying 2 nodes from each layout: 2022-03-24 at 11:52:37 | INFO Reconciling: [elastic-agent-sles, elastic-agent-ubuntu] 2022-03-24 at 11:52:37 | INFO Destroying: ogc-87ba30fc-elastic-agent-sles 2022-03-24 at 11:52:37 | INFO Destroying: ogc-51b971ad-elastic-agent-sles 2022-03-24 at 11:52:37 | INFO Destroying: ogc-b3befadc-elastic-agent-ubuntu 2022-03-24 at 11:52:37 | INFO Destroying: ogc-d54a5848-elastic-agent-ubuntu","title":"Syncing a deployment"},{"location":"user-guide/managing-nodes/#destroying-nodes","text":"OGC allows destroying of individual or a full blown cleanup. To remove a single node we run: $ ogc rm --by-name ogc-d7cd61a7-elastic-agent-ubuntu --force Or if we wanted to do a full teardown, run: $ ogc rm-all --force","title":"Destroying nodes"},{"location":"user-guide/providers/","text":"Providers \u00b6 In order to access a cloud provider, there are certain environment variables that need to be exposed for each. Each environment variable should be defined in .env file so it will be automatically loaded when running OGC. AWS \u00b6 AWS_ACCESS_KEY_ID AWS_SECRET_ACCESS_KEY AWS_REGION Google \u00b6 GOOGLE_APPLICATION_CREDENTIALS GOOGLE_APPLICATION_SERVICE_ACCOUNT GOOGLE_PROJECT GOOGLE_DATACENTER","title":"Providers"},{"location":"user-guide/providers/#providers","text":"In order to access a cloud provider, there are certain environment variables that need to be exposed for each. Each environment variable should be defined in .env file so it will be automatically loaded when running OGC.","title":"Providers"},{"location":"user-guide/providers/#aws","text":"AWS_ACCESS_KEY_ID AWS_SECRET_ACCESS_KEY AWS_REGION","title":"AWS"},{"location":"user-guide/providers/#google","text":"GOOGLE_APPLICATION_CREDENTIALS GOOGLE_APPLICATION_SERVICE_ACCOUNT GOOGLE_PROJECT GOOGLE_DATACENTER","title":"Google"},{"location":"user-guide/scripting/","text":"Scripting \u00b6 All deployments have the ability to execute scripts once a node becomes available. Before starting \u00b6 A couple of things to keep in mind: All scripts are executed in order based on the filenames. It is recommended to create scripts with a numbered prefix, for example: - scripts/ - 01-install-deps - 02-configure-services There is a special reserved filename teardown , if this file exists it will only be executed during a removal of a node. This is useful for any cleanup actions that may need to be run, such as removing test users, un-enrolling from a service, etc. Writing scripts \u00b6 Scripts can be written in any language, it is up to you to configure the nodes so that any runtimes and library dependencies are met on the target node for your script to execute in. One way to accomplish this is to create 01-setup-env bash script: #!/bin/bash echo \"Installing python3 on ubuntu\" sudo apt-get update sudo apt-get install -qyf python3 sudo pip install sh Then in subsequent scripts, using python3 is available. For example, in file 02-run-cmd-in-python : #!/usr/bin/env python3 import sh sh . ls ( '/' ) sh . cp ( '-a' , 'mydir' , 'anotherdir' ) Templating \u00b6 OGC provides some additional capabilities through templating. Under the hood python-mako is used for the parsing. With templating, you have the ability to query the underlying database to gather node information, a couple of modules are already exposed in the templates context: Var Description db Exposes access to the database models such as db.Node and db.Actions session Database session manager that allows querying the database, for example, session.query(db.Node).all() env Environment variables are made available through this key, env['USER'] #!/bin/bash echo \"\" % for node in session.query ( db.Node ) .filter ( db.Node.tags.contains ([ 'sles' ])) : echo \"[ID: ${ node .id } ] Name: ${ node .instance_name } || Connection: ${ node .username } @ ${ node .public_ip } || Provider: ${ node .provider } \" % endfor echo \"\" The runtime environment is also available within the template context. In one example, we can export the following into our .env file and reference those in the templates: OGC_ELASTIC_AGENT_VERSION OGC_ELASTIC_AGENT_SHA OGC_ELASTIC_AGENT_VERSION OGC_FLEET_URL OGC_FLEET_ENROLLMENT_TOKEN See the below example for downloading elastic-agent and enrolling it into a fleet server: #!/bin/bash <%namespace name = \"utils\" file = \"/functions.mako\" /> <% url = \"https://staging.elastic.co/%s-%s/downloads/beats/elastic-agent/elastic-agent-%s-linux-x86_64.tar.gz\" % ( env [ 'OGC_ELASTIC_AGENT_VERSION' ] , env [ 'OGC_ELASTIC_AGENT_SHA' ] , env [ 'OGC_ELASTIC_AGENT_VERSION' ]) %> ${ utils .setup_env() } ${ utils .install_pkgs([ 'nano' ]) } ${ utils .download(url, 'elastic-agent.tar.gz' ) } ${ utils .extract( 'elastic-agent.tar.gz' ) } mv elastic-agent- ${ env [ 'OGC_ELASTIC_AGENT_VERSION' ] } -linux-x86_64 elastic-agent cd elastic-agent && ./elastic-agent install -f --url = ${ env [ 'OGC_FLEET_URL' ] } --enrollment-token = ${ env [ 'OGC_FLEET_ENROLLMENT_TOKEN' ] } Reusable helpers \u00b6 In the above example we reference a file called /functions.mako this is just another template file that sits just outside of our defined scripts , for example, if our scripts is defined to be in scripts/my_ubuntu_deploy then this functions.mako will live at scripts/functions.mako . Alert This is good practice as you may have multiple layouts with different script directories for each and would like to store common functionality in a single place. Defining helper functions is straight forward, lets look at functions.mako for an example: ## Helper template functions downloading/extracting files <%def name = \"setup_env()\" > if ! test -f \"/usr/local/bin/pacapt\" ; then wget -O /usr/local/bin/pacapt https://github.com/icy/pacapt/raw/ng/pacapt chmod 755 /usr/local/bin/pacapt ln -sv /usr/local/bin/pacapt /usr/local/bin/pacman || true fi </%def> <%def name = \"install_pkgs(pkgs)\" > % for pkg in pkgs: pacapt install --noconfirm ${ pkg } % endfor </%def> <%def name = \"download(url, src_file)\" > wget -O ${ src_file } ${ url } </%def> <%def name = \"extract(src, dst=None)\" > % if dst: mkdir -p ${ dst } tar -xvf ${ src } -C ${ dst } % else : tar -xvf ${ src } % endif </%def> Each %def section defines a function block that when called with any necessary arguments will output that data into the scripts with all necessary translations handled. You can see the usage of these functions in the previous example for installing elastic-agent. It is worth the time to visit Mako's website and learn about its feature set, particularly namespaces and defs and blocks .","title":"Scripting"},{"location":"user-guide/scripting/#scripting","text":"All deployments have the ability to execute scripts once a node becomes available.","title":"Scripting"},{"location":"user-guide/scripting/#before-starting","text":"A couple of things to keep in mind: All scripts are executed in order based on the filenames. It is recommended to create scripts with a numbered prefix, for example: - scripts/ - 01-install-deps - 02-configure-services There is a special reserved filename teardown , if this file exists it will only be executed during a removal of a node. This is useful for any cleanup actions that may need to be run, such as removing test users, un-enrolling from a service, etc.","title":"Before starting"},{"location":"user-guide/scripting/#writing-scripts","text":"Scripts can be written in any language, it is up to you to configure the nodes so that any runtimes and library dependencies are met on the target node for your script to execute in. One way to accomplish this is to create 01-setup-env bash script: #!/bin/bash echo \"Installing python3 on ubuntu\" sudo apt-get update sudo apt-get install -qyf python3 sudo pip install sh Then in subsequent scripts, using python3 is available. For example, in file 02-run-cmd-in-python : #!/usr/bin/env python3 import sh sh . ls ( '/' ) sh . cp ( '-a' , 'mydir' , 'anotherdir' )","title":"Writing scripts"},{"location":"user-guide/scripting/#templating","text":"OGC provides some additional capabilities through templating. Under the hood python-mako is used for the parsing. With templating, you have the ability to query the underlying database to gather node information, a couple of modules are already exposed in the templates context: Var Description db Exposes access to the database models such as db.Node and db.Actions session Database session manager that allows querying the database, for example, session.query(db.Node).all() env Environment variables are made available through this key, env['USER'] #!/bin/bash echo \"\" % for node in session.query ( db.Node ) .filter ( db.Node.tags.contains ([ 'sles' ])) : echo \"[ID: ${ node .id } ] Name: ${ node .instance_name } || Connection: ${ node .username } @ ${ node .public_ip } || Provider: ${ node .provider } \" % endfor echo \"\" The runtime environment is also available within the template context. In one example, we can export the following into our .env file and reference those in the templates: OGC_ELASTIC_AGENT_VERSION OGC_ELASTIC_AGENT_SHA OGC_ELASTIC_AGENT_VERSION OGC_FLEET_URL OGC_FLEET_ENROLLMENT_TOKEN See the below example for downloading elastic-agent and enrolling it into a fleet server: #!/bin/bash <%namespace name = \"utils\" file = \"/functions.mako\" /> <% url = \"https://staging.elastic.co/%s-%s/downloads/beats/elastic-agent/elastic-agent-%s-linux-x86_64.tar.gz\" % ( env [ 'OGC_ELASTIC_AGENT_VERSION' ] , env [ 'OGC_ELASTIC_AGENT_SHA' ] , env [ 'OGC_ELASTIC_AGENT_VERSION' ]) %> ${ utils .setup_env() } ${ utils .install_pkgs([ 'nano' ]) } ${ utils .download(url, 'elastic-agent.tar.gz' ) } ${ utils .extract( 'elastic-agent.tar.gz' ) } mv elastic-agent- ${ env [ 'OGC_ELASTIC_AGENT_VERSION' ] } -linux-x86_64 elastic-agent cd elastic-agent && ./elastic-agent install -f --url = ${ env [ 'OGC_FLEET_URL' ] } --enrollment-token = ${ env [ 'OGC_FLEET_ENROLLMENT_TOKEN' ] }","title":"Templating"},{"location":"user-guide/scripting/#reusable-helpers","text":"In the above example we reference a file called /functions.mako this is just another template file that sits just outside of our defined scripts , for example, if our scripts is defined to be in scripts/my_ubuntu_deploy then this functions.mako will live at scripts/functions.mako . Alert This is good practice as you may have multiple layouts with different script directories for each and would like to store common functionality in a single place. Defining helper functions is straight forward, lets look at functions.mako for an example: ## Helper template functions downloading/extracting files <%def name = \"setup_env()\" > if ! test -f \"/usr/local/bin/pacapt\" ; then wget -O /usr/local/bin/pacapt https://github.com/icy/pacapt/raw/ng/pacapt chmod 755 /usr/local/bin/pacapt ln -sv /usr/local/bin/pacapt /usr/local/bin/pacman || true fi </%def> <%def name = \"install_pkgs(pkgs)\" > % for pkg in pkgs: pacapt install --noconfirm ${ pkg } % endfor </%def> <%def name = \"download(url, src_file)\" > wget -O ${ src_file } ${ url } </%def> <%def name = \"extract(src, dst=None)\" > % if dst: mkdir -p ${ dst } tar -xvf ${ src } -C ${ dst } % else : tar -xvf ${ src } % endif </%def> Each %def section defines a function block that when called with any necessary arguments will output that data into the scripts with all necessary translations handled. You can see the usage of these functions in the previous example for installing elastic-agent. It is worth the time to visit Mako's website and learn about its feature set, particularly namespaces and defs and blocks .","title":"Reusable helpers"},{"location":"user-guide/sharing/","text":"Sharing Environment \u00b6 OGC provides a way to share an environment with another user, they are facilitated through the ogc export-env and ogc import-env commands. This document will walk through how to accomplish this plus any caveats to be aware of. Exporting \u00b6 First, we need to export the current environment. This will dump the database of node information, the environment variables used in the deployment, and will include the public ssh key of the shared user on all nodes. The person you are sharing with should provide you with a passwordless public ssh key. This can either be the contents of the public ssh key or a GitHub username that has public ssh keys associated with their profile. $ ogc export-env How would you like to import the users public ssh key [ github/manual ] ( github ) : Please enter your Github username: adam-stokes Once the export is complete it will print out the results including some instructions on what to do next Importing \u00b6 Once the user has both the database ogc-dump.sql and the environment ogc-env.json they will need to import that on their machine. Caution If the user already has ogc installed and a deployment created, they may want to keep that information intact. To avoid overwriting the users database during import make sure to prefix the command with POSTGRES_DB : $ POSTGRES_DB = \"ogcopydb\" ogc import-env Also, the import command will automatically write to files ogc.yml and .env . If those files already exist in the directory where the import command is run then you will want to do one of two things: Create a backup of those files Run ogc import-env from a different directory, perhaps creating a temporary directory for working with the shared environment. To do the import run the following: $ POSTGRES_DB = \"ogcopydb\" ogc import-env --env-file ../ogc-env.json \\ --db-file ../ogc-dump.sql \\ --private-ssh-key ~/.ssh/id_rsa \\ --public-ssh-key ~/.ssh/id_rsa.pub The private-ssh-key and public-ssh-key should be the locations of the keys associated with the one shared during export. Once import is complete, you can access this environment: $ POSTGRES_DB = \"ogcopydb\" ogc ls Warning The cloud credentials are not copied over, you will need to setup your credentials again in the newly created .env . You will need to make sure that the correct project/region is set and accessible by your cloud account.","title":"Sharing"},{"location":"user-guide/sharing/#sharing-environment","text":"OGC provides a way to share an environment with another user, they are facilitated through the ogc export-env and ogc import-env commands. This document will walk through how to accomplish this plus any caveats to be aware of.","title":"Sharing Environment"},{"location":"user-guide/sharing/#exporting","text":"First, we need to export the current environment. This will dump the database of node information, the environment variables used in the deployment, and will include the public ssh key of the shared user on all nodes. The person you are sharing with should provide you with a passwordless public ssh key. This can either be the contents of the public ssh key or a GitHub username that has public ssh keys associated with their profile. $ ogc export-env How would you like to import the users public ssh key [ github/manual ] ( github ) : Please enter your Github username: adam-stokes Once the export is complete it will print out the results including some instructions on what to do next","title":"Exporting"},{"location":"user-guide/sharing/#importing","text":"Once the user has both the database ogc-dump.sql and the environment ogc-env.json they will need to import that on their machine. Caution If the user already has ogc installed and a deployment created, they may want to keep that information intact. To avoid overwriting the users database during import make sure to prefix the command with POSTGRES_DB : $ POSTGRES_DB = \"ogcopydb\" ogc import-env Also, the import command will automatically write to files ogc.yml and .env . If those files already exist in the directory where the import command is run then you will want to do one of two things: Create a backup of those files Run ogc import-env from a different directory, perhaps creating a temporary directory for working with the shared environment. To do the import run the following: $ POSTGRES_DB = \"ogcopydb\" ogc import-env --env-file ../ogc-env.json \\ --db-file ../ogc-dump.sql \\ --private-ssh-key ~/.ssh/id_rsa \\ --public-ssh-key ~/.ssh/id_rsa.pub The private-ssh-key and public-ssh-key should be the locations of the keys associated with the one shared during export. Once import is complete, you can access this environment: $ POSTGRES_DB = \"ogcopydb\" ogc ls Warning The cloud credentials are not copied over, you will need to setup your credentials again in the newly created .env . You will need to make sure that the correct project/region is set and accessible by your cloud account.","title":"Importing"},{"location":"user-guide/upgrading/","text":"Upgrading OGC \u00b6 Upgrades should be relatively easy, the only item to be careful of is the database. OGC tries to keep database changes to a minimum, however, in some cases it is unavoidable and the database schema needs to be updated. This will effect users who: Have an active deployment which means objects stored in the database Upgraded OGC from a previous version without tearing down the known deployments To help with the upgrade process, OGC provides a cli command to update your current database schema. Once OGC is upgraded to a new version run the following: $ ogc db-migrate This will sync up your current database with the latest schema from OGC. Note Only when the database schema changes will you need to run this. Communication about database changes will be made in the release notes.","title":"Upgrading"},{"location":"user-guide/upgrading/#upgrading-ogc","text":"Upgrades should be relatively easy, the only item to be careful of is the database. OGC tries to keep database changes to a minimum, however, in some cases it is unavoidable and the database schema needs to be updated. This will effect users who: Have an active deployment which means objects stored in the database Upgraded OGC from a previous version without tearing down the known deployments To help with the upgrade process, OGC provides a cli command to update your current database schema. Once OGC is upgraded to a new version run the following: $ ogc db-migrate This will sync up your current database with the latest schema from OGC. Note Only when the database schema changes will you need to run this. Communication about database changes will be made in the release notes.","title":"Upgrading OGC"},{"location":"user-guide/cookbook/template-access-node-info/","text":"Accessing node information \u00b6 Current node \u00b6 In template files you have access to the node that is currently being rendered prior to those scripts being uploaded. Below is an example of querying the current node's information and outputting it to a file: #!/bin/bash echo \"### CURRENT NODE\" >> node_info.txt echo \"[ID: ${ node .id } ] Name: ${ node .instance_name } || Connection: ${ node .username } @ ${ node .public_ip } || Provider: ${ node .provider } \" >> node_info.txt echo \"### CURRENT NODE\" >> node_info.txt Save this file in the location of your defined scripts and give it a indexed name of where in the order it should be executed, for example, 01-show-node-info 1 . All nodes \u00b6 In some cases you may need to grab information from another node in the deployment, for example, a second node running Kibana in which the first node needs to perform some kind of API calls against it. We can accomplish this using the db and session modules that's exposed in our templates. Create a file 02-curl-remote with the following: #!/bin/bash sudo pip install httpie KIBANA_HOST = ${ session .query(db.Node).filter(db.Node.instance_name.contains([ \"kibana\" ]).first() or '' ) } http -a username:passsword -f GET https:// $KIBANA_HOST :5601/fleet/setup kbn-xsrf:ogc See the Scripting documentation for ordering of files. \u21a9","title":"Access node info"},{"location":"user-guide/cookbook/template-access-node-info/#accessing-node-information","text":"","title":"Accessing node information"},{"location":"user-guide/cookbook/template-access-node-info/#current-node","text":"In template files you have access to the node that is currently being rendered prior to those scripts being uploaded. Below is an example of querying the current node's information and outputting it to a file: #!/bin/bash echo \"### CURRENT NODE\" >> node_info.txt echo \"[ID: ${ node .id } ] Name: ${ node .instance_name } || Connection: ${ node .username } @ ${ node .public_ip } || Provider: ${ node .provider } \" >> node_info.txt echo \"### CURRENT NODE\" >> node_info.txt Save this file in the location of your defined scripts and give it a indexed name of where in the order it should be executed, for example, 01-show-node-info 1 .","title":"Current node"},{"location":"user-guide/cookbook/template-access-node-info/#all-nodes","text":"In some cases you may need to grab information from another node in the deployment, for example, a second node running Kibana in which the first node needs to perform some kind of API calls against it. We can accomplish this using the db and session modules that's exposed in our templates. Create a file 02-curl-remote with the following: #!/bin/bash sudo pip install httpie KIBANA_HOST = ${ session .query(db.Node).filter(db.Node.instance_name.contains([ \"kibana\" ]).first() or '' ) } http -a username:passsword -f GET https:// $KIBANA_HOST :5601/fleet/setup kbn-xsrf:ogc See the Scripting documentation for ordering of files. \u21a9","title":"All nodes"}]}